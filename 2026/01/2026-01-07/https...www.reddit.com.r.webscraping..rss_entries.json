[
    {
        "age": null,
        "album": "",
        "author": "/u/franik33",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T20:05:34.222779+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T20:01:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I started getting into web scraping about 3\u20134 days ago. I already have some solid experience with Python, and my first scraping project was a public website. I managed to collect around 7,000 records and everything worked as expected.</p> <p>I\u2019m curious whether this is considered a decent start for someone new to scraping, or if it\u2019s fairly basic stuff.<br/> Also, I\u2019d like to hear honest opinions: is web scraping still worth investing time in today (for projects, automation, or monetization), or is it becoming a waste of time due to market saturation and restrictions?</p> <p>Any real-world experiences or insights would be appreciated.</p> <p>Thanks in advance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/franik33\"> /u/franik33 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6pxwn/just_started_web_scraping_is_this_a_good_start/\">[link]</a></span> &#32; <span><a href",
        "id": 4499838,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6pxwn/just_started_web_scraping_is_this_a_good_start",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Just Started Web Scraping \u2014 Is This a Good Start?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MetroidsSuffering",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T20:05:34.362675+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T19:39:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to do a multi step process with regards to a site with a paywall and I would like to know practical tips and the legality of this described process. Essentially</p> <ol> <li><p>I get a subscription to ESPN Insider.</p></li> <li><p>I use that subscription to scrape ESPN Insider opinion articles.</p></li> <li><p>I use an LLM to extract sentiment from these opinion articles.</p></li> <li><p>I then include those sentiment measures in a dataset I run a regression on.</p></li> </ol> <p>Is this process legal and what are the best legal opinions on this? And if it is legal, what do I need to specifically do about scraping a paywalled site that differs from a site without a paywall.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MetroidsSuffering\"> /u/MetroidsSuffering </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6pclf/webscraping_a_site_with_a_paywall_while_having_a/\">[link]</a></span",
        "id": 4499839,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6pclf/webscraping_a_site_with_a_paywall_while_having_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Webscraping a site with a paywall while having a subscription myself",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/hecarfen",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T16:39:11.228703+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T16:11:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi! I&#39;m building a small tracking tool for ordered stuffs using Playwright. I&#39;m not calling the endpoint directly but I load the public tracking page and use the network responses to capture the json payload.</p> <p>What&#39;s confusing me is that during a single page load I often see both a 429 Too Many Requests response on the end point&#39;s request with a captcha puzzle header (no json body in DevTools Response tab), and shortly after, I see 200 OK response (same endpoint) that does return the json I need. So it looks like the WAF/anti-bot layer uses the 429/captcha signal, but the page still ends up getting a successful 200 payload afterwards. I have not hit the captcha page so far but when I want to keep track of multiple items parallely I believe that it might be a problem. I have not seen such a response bundle together before so what&#39;s the best pattern to handle this reliably? Also what kind of approach can it be done if captcha b",
        "id": 4497874,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6jitx/429_captcha_followed_by_200_json_in_same_request",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "429 captcha followed by 200 json in same request",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sandwich_stevens",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T15:32:49.072066+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T15:27:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>im curious what data quality concerns people face when scraping lots of data or perhaps mid sized amounts of data. is it mostly around schema drift or are there more painful issues? would it be something you\u2019d outsource for checks or does the solution have to be internal, and do you already have it. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sandwich_stevens\"> /u/sandwich_stevens </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6ibd5/whats_your_biggest_data_quality_concern_would_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6ibd5/whats_your_biggest_data_quality_concern_would_you/\">[comments]</a></span>",
        "id": 4497282,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6ibd5/whats_your_biggest_data_quality_concern_would_you",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s your biggest data quality concern, would you outsource?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Edsaur",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T14:25:36.982981+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T14:12:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello! </p> <p>I am currently using nodriver/zendrive as my web scraper and also form automation. While it works best most especially for antibot/captcha detection, are there any other alternatives that does what they do? </p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Edsaur\"> /u/Edsaur </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6ge5w/any_nodriverzendrive_alternatives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6ge5w/any_nodriverzendrive_alternatives/\">[comments]</a></span>",
        "id": 4496610,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6ge5w/any_nodriverzendrive_alternatives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any nodriver/zendrive alternatives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sandwich_stevens",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T12:12:29.855127+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T11:54:28+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>im curious what data quality concerns people face when scraping lots of data or perhaps mid sized amounts of data. is it mostly around schema drift or are there more painful issues? would it be something you\u2019d outsource for checks or does the solution have to be internal, and do you already have it. I\u2019m just curious what I\u2019d be able to help from a data quality perspective with issues that crop up frequently with scraped data at scale</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sandwich_stevens\"> /u/sandwich_stevens </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6df8z/whats_your_biggest_data_quality_concern_would_you/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6df8z/whats_your_biggest_data_quality_concern_would_you/\">[comments]</a></span>",
        "id": 4495553,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6df8z/whats_your_biggest_data_quality_concern_would_you",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What\u2019s your biggest data quality concern, would you outsource?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Eastern_Ad_9018",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-07T11:06:53.410054+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-07T08:41:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Encountered a problem, seeking advice: When using curl-cffi to make a large number of requests to a certain website, the site records this fingerprint and returns a 403. At this point, switching to other libraries like requests and aiohttp allows requests to go through normally, but when the concurrency increases or after a few minutes, all requests also return 403. </p> <p>Any other ideas, or are there other libraries that can solve this problem? </p> <p>PS: It&#39;s not related to request headers or IP. There is a corresponding IP pool and cookie generation logic. Currently using requests-go with browser TLS, which causes other issues.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Eastern_Ad_9018\"> /u/Eastern_Ad_9018 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6a7ne/website_risk_control/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6a7ne",
        "id": 4495078,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6a7ne/website_risk_control",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Website Risk Control",
        "vote": 0
    }
]