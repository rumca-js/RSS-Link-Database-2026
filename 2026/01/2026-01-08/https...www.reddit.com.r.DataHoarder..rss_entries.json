[
    {
        "age": null,
        "album": "",
        "author": "/u/matevoun",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-09T00:47:57.924313+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T23:55:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi !</p> <p>I&#39;m Mathieu, founder of WDA, a French non-profit I started back in 1988. We&#39;ve been rescuing vintage computers, consoles, and tech from destruction for nearly four decades.</p> <p><strong>What we do:</strong></p> <ul> <li>Free computer/console pickup across France (and Belgium)</li> <li>Restoration and redistribution to collectors/museums</li> <li>Massive driver &amp; manual database online since 1996</li> <li>Currently managing 500m\u00b3 of tech from the 1950s to today</li> </ul> <p><strong>Some highlights from our collection:</strong></p> <ul> <li>Entire BIOS association collection (Rouen, 2019)</li> <li>Major part of Paris Computer Museum collection (2017)</li> <li>Everything from early mainframes to 90s gaming rigs</li> </ul> <p>We&#39;re less than 10 volunteers doing this entirely for free - no subsidies, just passion. Recently launched a WhatsApp channel to share restoration projects, rare finds, and retro gaming content.</p> <p>",
        "id": 4511639,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7semj/38_years_preserving_frances_digital_heritage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "38 years preserving France's digital heritage - 500m\u00b3 of vintage tech saved from landfills (volunteers needed!)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/haronclv",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T22:30:44.589674+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T22:06:20+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all, I&#39;ve recently made my NAS server, I want to use Kopia for backups, I have several drives connected to the server and I want to backup all of them to corresponding drives like</p> <p>2TB -&gt; 2TB_backup</p> <p>4TB -&gt; $TB_backup</p> <p>The issue is Kopia needs to be connected to the repository to create a backup. Is there any way to make KopiaUI connected to 2 repositories, or open 2 instances of kopiaUI - each connected to different repository? So auto backups can run without switching repositories.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/haronclv\"> /u/haronclv </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7pnh2/kopia_kopiaui/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7pnh2/kopia_kopiaui/\">[comments]</a></span>",
        "id": 4510756,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7pnh2/kopia_kopiaui",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Kopia / KopiaUI",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Captain_Candyy",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T20:13:49.517772+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T19:32:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Synology has some good Nas systems, but they started to show signs of enshittification with the &quot;we only accept approved drives&quot;. So i was wondering, what&#39;s the best set up for a starter for a Nas/data hoarding.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Captain_Candyy\"> /u/Captain_Candyy </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7lheq/best_starter_nas_to_start_hoarding/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7lheq/best_starter_nas_to_start_hoarding/\">[comments]</a></span>",
        "id": 4509668,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7lheq/best_starter_nas_to_start_hoarding",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best starter Nas to start hoarding",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DarkJoney",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T20:13:49.652544+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T19:07:40+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7ksg6/whats_inside_of_22tb_wd_elements/\"> <img src=\"https://b.thumbs.redditmedia.com/_XxaCD-_CIkkqGg1RHRTsOKkFH3hctuKudLnO3onUVY.jpg\" alt=\"What's inside of 22TB WD Elements\" title=\"What's inside of 22TB WD Elements\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/720btb72c6cg1.png?width=1159&amp;format=png&amp;auto=webp&amp;s=dd1048a74f5d0683f734734a71be6ad2b7c70222\">https://preview.redd.it/720btb72c6cg1.png?width=1159&amp;format=png&amp;auto=webp&amp;s=dd1048a74f5d0683f734734a71be6ad2b7c70222</a></p> <p>Hi guys,</p> <p>Grabbed two of those yesterday, both are having WD Gold inside. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DarkJoney\"> /u/DarkJoney </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7ksg6/whats_inside_of_22tb_wd_elements/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHo",
        "id": 4509669,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7ksg6/whats_inside_of_22tb_wd_elements",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/_XxaCD-_CIkkqGg1RHRTsOKkFH3hctuKudLnO3onUVY.jpg",
        "title": "What's inside of 22TB WD Elements",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Belvyzep",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T21:24:24.836794+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T17:50:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, I do a lot of scanning and working with documents. At my desk right now I have an Epson V800, a CZUR Pro, a Canon Eos Rebel SL1, and a smartphone with the Adobe Scan app installed. Between these tools, I can get an excellent scan of anything sized between a Kodachrome slide and an A4 document, as well as a &quot;good enough to have a legible, OCR&#39;d copy backed up to our network&quot; copy of most books.</p> <p>However, I have a fair quantity of documents that are size A1 and above. These include bound newspapers, posters, maps, blueprints, et cetera. Some of these I&#39;m able to stitch together with enough time and patience, but beyond a certain point that&#39;s beyond my limited photo manipulation skills.</p> <p>So with that said, what are some good solutions for scanning these beasts? I&#39;m open to suggestions for flatbed scanners, overhead scanners, or whatever else can get the job done. The cheaper the better. If I&#39;m able to use too",
        "id": 4510201,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7il8k/good_newspapermapother_huge_document_scanning",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Good newspaper/Map/Other Huge Document scanning solutions?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Electronic-Clock5867",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T17:56:00.423427+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T17:00:17+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7h6og/i_wonder_if_there_is_anything_goodrare_on_these/\"> <img src=\"https://preview.redd.it/yr7re5vfp5cg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=76ccd8fd047de3f6e8cab925f77ae25966e71a32\" alt=\"I wonder if there is anything good/rare on these tapes\" title=\"I wonder if there is anything good/rare on these tapes\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic-Clock5867\"> /u/Electronic-Clock5867 </a> <br/> <span><a href=\"https://i.redd.it/yr7re5vfp5cg1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7h6og/i_wonder_if_there_is_anything_goodrare_on_these/\">[comments]</a></span> </td></tr></table>",
        "id": 4508276,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7h6og/i_wonder_if_there_is_anything_goodrare_on_these",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/yr7re5vfp5cg1.jpeg?width=640&crop=smart&auto=webp&s=76ccd8fd047de3f6e8cab925f77ae25966e71a32",
        "title": "I wonder if there is anything good/rare on these tapes",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/fulthrottlejazzhands",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T17:55:59.421284+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:58:35+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>A gym buddy has a stockpile of used and new im-box XPG GAMMIX S70 BLADE PCIe M.2s. He&#39;s offered to sell me 2x 2TB and 2x 1TB of the new ones for \u00a3360. </p> <p>I&#39;m looking to use three of them in my primary PC as regular game and application install storage coupled with a faster Gen5 drive used for OS, and another in a PS5. Nothing would be irreplaceable on them, I have a NAS with backup for that.</p> <p>How are these drives? Is \u00a3360 a good deal for the lot? I have an XPG SX8200 that&#39;s been going strong in a secondary PC for 5 years now (understanding these are largely different drives)...</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/fulthrottlejazzhands\"> /u/fulthrottlejazzhands </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7h4z6/opportunity_to_buy_xpg_gammix_s70_blade_pcie_m2s/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7h4z6",
        "id": 4508275,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7h4z6/opportunity_to_buy_xpg_gammix_s70_blade_pcie_m2s",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Opportunity to buy XPG GAMMIX S70 BLADE PCIe M.2s... Am I good with these?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/DiogoAlmeida97",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T17:55:58.140873+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:50:29+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7gwtr/sandisk_extreme_pro_ssd_design_update/\"> <img src=\"https://preview.redd.it/cow0f72rm5cg1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bef8e6b234831ef53f01f1114e5216d5ba53fb03\" alt=\"Sandisk Extreme Pro SSD design update\" title=\"Sandisk Extreme Pro SSD design update\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Don&#39;t know when this revision started, but from the latest batch I got, only 2 out of 8 SSDs still had NVMe drives connected to a USB controller. All other drives now feature a smaller board that shares both the controller and the NAND package.<br/> And the thermal solution is now just some conductive foil around the board instead of a thermal pad </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DiogoAlmeida97\"> /u/DiogoAlmeida97 </a> <br/> <span><a href=\"https://i.redd.it/cow0f72rm5cg1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.redd",
        "id": 4508274,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7gwtr/sandisk_extreme_pro_ssd_design_update",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/cow0f72rm5cg1.jpeg?width=320&crop=smart&auto=webp&s=bef8e6b234831ef53f01f1114e5216d5ba53fb03",
        "title": "Sandisk Extreme Pro SSD design update",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/codered11343",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:38.176828+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:43:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a 350TB unraid server with spinning drives, but I now have around 100+ (and growing) 2.5in SSDs I&#39;ve taken from work and would love to put some of them to use. </p> <p>Any advice on a high density JBOD solution? I&#39;m also happy to DIY some of this as well. So far I&#39;ve seen 20 2.5in drive JBODs for sale used locally for a really good price, but like I said, I&#39;m interested in density. (as many drives in as small of a space as possible)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/codered11343\"> /u/codered11343 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7gpza/high_density_jbod_or_diy_jbod_recommendationideas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7gpza/high_density_jbod_or_diy_jbod_recommendationideas/\">[comments]</a></span>",
        "id": 4507652,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7gpza/high_density_jbod_or_diy_jbod_recommendationideas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "High density JBOD or DIY JBOD recommendation/ideas for 2.5in drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/zebus_0",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:38.464355+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:42:47+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve seen some people doing some really cool projects with pi machines to build a custom &quot;TV channel&quot; with old shows and commercials from certain eras. My question is where to even start looking for collections of commercials. If I need to specify further I am wanting to look for mostly kids toys commercials and Halloween ones (for nostalgia) to work on this. I imagine it&#39;s hard to get and people didn&#39;t archive such things?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/zebus_0\"> /u/zebus_0 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7gpcy/commercial_repositories/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7gpcy/commercial_repositories/\">[comments]</a></span>",
        "id": 4507653,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7gpcy/commercial_repositories",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Commercial Repositories?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/643310",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:37.948476+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:24:59+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7g82x/n150_6_sata_or_i58500_8_sata_for_a_nas_and_plex/\"> <img src=\"https://b.thumbs.redditmedia.com/bcYo-vMU-ssFXf03zZabeuG-XTlzhOHrp5JG6J9ENOA.jpg\" alt=\"N150 &amp; 6 Sata or I5-8500 &amp; 8+ Sata for a NAS and Plex server?\" title=\"N150 &amp; 6 Sata or I5-8500 &amp; 8+ Sata for a NAS and Plex server?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have been looking at 2 CPU + MB combos while planning to build my first Nas. Both are ITX boards from AliExpress which seem to have good ratings and they both cost about 200\u20ac.</p> <p>I am torn between which to choose because both have their pros and cons. CPU performance should be fine with both of them, but I am unsure which to choose.</p> <p>N150 board Pros: -10Gig Lan -low profile cooler -integrated cooler -much lower power consumption -can fully populate a Jonsbo N2</p> <p>Cons: -only 6 Sata 3.0 ports with no space to upgrade (probably possible with one of t",
        "id": 4507651,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7g82x/n150_6_sata_or_i58500_8_sata_for_a_nas_and_plex",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/bcYo-vMU-ssFXf03zZabeuG-XTlzhOHrp5JG6J9ENOA.jpg",
        "title": "N150 & 6 Sata or I5-8500 & 8+ Sata for a NAS and Plex server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Leverpostei414",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:38.625433+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:22:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a raid6 array, one of my drives timed out and the array degraded, i rebuilt the array, no problems. But my suspicion is that this is only the first sign of trouble, ir will get worse from here. Any experience with this? Can a time out from a disk be a one time thing or should I probably get a new disk? (The reason i ask i partly price based, checked a replacement, how can a HDD cost almost twice as much now as I paid 6 years ago?!?, can i assume prices will rise or fall the next 6 months?)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Leverpostei414\"> /u/Leverpostei414 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7g5bj/hard_drive_timed_out_and_array_degraded_chance_of/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7g5bj/hard_drive_timed_out_and_array_degraded_chance_of/\">[comments]</a></span>",
        "id": 4507654,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7g5bj/hard_drive_timed_out_and_array_degraded_chance_of",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hard drive timed out and array degraded, chance of it being a one time thing?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AidenValentine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:38.836697+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:11:55+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7fvel/github_dollsofinkyolandi_yolandi_is_a_nodebased/\"> <img src=\"https://external-preview.redd.it/8uthAR20OQeiiRDrMchRls6LRy1C5oktQkQbuvfM-QI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d3d9b20953aeebb0b3af97e51a0ce4625fdf9c7\" alt=\"GitHub - dollsofink/yolandi: YOLANDI is a node-based Graph Function workflow automation platform for Wordpress. Write workflows and automations for WP! Includes TONS of integrations!\" title=\"GitHub - dollsofink/yolandi: YOLANDI is a node-based Graph Function workflow automation platform for Wordpress. Write workflows and automations for WP! Includes TONS of integrations!\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><h1>Yolandi - Graph Function &amp; Workflows</h1> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AidenValentine\"> /u/AidenValentine </a> <br/> <span><a href=\"https://github.com/dollsofink/yolandi\">[link]</a></span> &#32; <spa",
        "id": 4507655,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7fvel/github_dollsofinkyolandi_yolandi_is_a_nodebased",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/8uthAR20OQeiiRDrMchRls6LRy1C5oktQkQbuvfM-QI.png?width=640&crop=smart&auto=webp&s=5d3d9b20953aeebb0b3af97e51a0ce4625fdf9c7",
        "title": "GitHub - dollsofink/yolandi: YOLANDI is a node-based Graph Function workflow automation platform for Wordpress. Write workflows and automations for WP! Includes TONS of integrations!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ch3mn3y",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:39.132162+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:11:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I&#39;m thinking which road to take. I&#39;m moving (dunno if rn, in a month or so) from 17 HHDs (15x3TB + 2x4TB) as a mergerFS+SnapRAID. However it hurts my power bill, so I want to downsize to 3 drives - same setup, but 2x16+18.</p> <p>Looking at Allegro (polish eBay), eBay and Amazon I choose recertified Seagate Exos drives. I found 16 TB ones in both X and X2 variant and 18 TB one only as X2. Is there big difference and should I choose X2 for both?<br/> If i&#39;ll go with X than it&#39;ll be X24 ST16000NM000H, if with X2 than 2X18 ST16000NM0092 and 2X18 ST18000NM0092.</p> <p>And question for user if Exos, Exos X and/or Exos X2 are supported by Proxmox and OMV? I believe that yes, but seen info from some sellers that I should make sure that my OS support Exos drives. Don&#39;t think that&#39;s the case, SATA drive is SATA drive, right?</p> <p>I&#39;m asking for Proxmox and OMV cos I&#39;m still not sure if I&#39;ll go bare metal OMV (that&#39;s",
        "id": 4507656,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7fusg/seagate_exos_x_vs_x2_for_home_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate Exos X vs X2 for home NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AidenValentine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:39.374480+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:11:02+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7fulh/bbs_video_cms_video_content_management_software/\"> <img src=\"https://external-preview.redd.it/iUztMPWbVrkCHCDbKk7AP3Zfy5BZpE5ZjlkgXHxUFUQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cae66d3c5affcf78018aac7aeca222197a1e2e1a\" alt=\"BB's Video CMS (Video Content Management software)\" title=\"BB's Video CMS (Video Content Management software)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Complete replacement for Windows Explorer or File Finder</p> <p>A file-level video CMS with <strong>hover scrubbing</strong>, <strong>XMP/TXT sidecars</strong>, <strong>right\u2011click transcode/upload actions</strong>, <strong>color\u2011coded thumbnails</strong> (full/trailer/footage), <strong>search</strong>, and a <strong>built-in video player</strong>.</p> <p><a href=\"https://github.com/dollsofink/bbs-video-cms/blob/main/Screenshot%202025-12-11%20214617.png\"></a></p> <h1>Quick start</h1> <p><a href=\"https://github.com/do",
        "id": 4507657,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7fulh/bbs_video_cms_video_content_management_software",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/iUztMPWbVrkCHCDbKk7AP3Zfy5BZpE5ZjlkgXHxUFUQ.png?width=640&crop=smart&auto=webp&s=cae66d3c5affcf78018aac7aeca222197a1e2e1a",
        "title": "BB's Video CMS (Video Content Management software)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AidenValentine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:39.609560+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:09:21+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7fsyg/stealth_api_hide_yo_455/\"> <img src=\"https://external-preview.redd.it/_vYM0vsZVkxmOnsFZymTfD3kUbHqLptNyIfcmbejNE4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=71db7fb7d9e30b47738cf285e1675ed712be9ae3\" alt=\"Stealth API - Hide yo 455\" title=\"Stealth API - Hide yo 455\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Uber Stealth wrapper for Puppeteer, ExpressJS, and Fetch + (weighted) GEO-mapping proxies, (weighted) Device Emulation and anything else you&#39;d need to hide your 4$$ and bots activities.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AidenValentine\"> /u/AidenValentine </a> <br/> <span><a href=\"https://github.com/dollsofink/stealth-api\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7fsyg/stealth_api_hide_yo_455/\">[comments]</a></span> </td></tr></table>",
        "id": 4507658,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7fsyg/stealth_api_hide_yo_455",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/_vYM0vsZVkxmOnsFZymTfD3kUbHqLptNyIfcmbejNE4.png?width=640&crop=smart&auto=webp&s=71db7fb7d9e30b47738cf285e1675ed712be9ae3",
        "title": "Stealth API - Hide yo 455",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AidenValentine",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T16:48:40.167646+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T16:07:25+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7fr2j/github_dollsofinkalienprincessbymistervalentine/\"> <img src=\"https://external-preview.redd.it/Ew7C7pXZCZpzngBcOvKENDlJzLVto0kk8WZl658RjvM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea62e7ccf77f89315090f2aeca317a4bade8fe42\" alt=\"GitHub - dollsofink/Alien-Princess-by-Mister-Valentine: Discover ALIEN TECHNOLOGY \ud83d\udc7d this is my Emergency backup of this important album\" title=\"GitHub - dollsofink/Alien-Princess-by-Mister-Valentine: Discover ALIEN TECHNOLOGY \ud83d\udc7d this is my Emergency backup of this important album\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>This album explains Aliens using rap &amp; hip-hop.<br/> Just listen to the lyrical genius and tell me he *isn&#39;t an Alien. (And outer space beings spreading legs across the U.S.A.)</p> <p>- Aiden Valentine<br/> From XNXX fame<br/> Location: Tampa, Florida (Hillsborough County)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www",
        "id": 4507659,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7fr2j/github_dollsofinkalienprincessbymistervalentine",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/Ew7C7pXZCZpzngBcOvKENDlJzLVto0kk8WZl658RjvM.png?width=640&crop=smart&auto=webp&s=ea62e7ccf77f89315090f2aeca317a4bade8fe42",
        "title": "GitHub - dollsofink/Alien-Princess-by-Mister-Valentine: Discover ALIEN TECHNOLOGY \ud83d\udc7d this is my Emergency backup of this important album",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Hauptfeldwebel",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T14:35:11.091109+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T14:30:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Im using the part of an old pc from me:</p> <ul> <li>i5 8600K 6x 3.60GHz</li> <li>ASRock Z370 Pro</li> <li>16GB Crucial DDR4-2666</li> <li>(+ 3x 16TB HDDs + 2 SSDs)</li> </ul> <p>And the old PSUs fan (be quiet! L8 500W) is very noisy, so i want to invest in a better more silent PSU in idle and probably lower watt in idle.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Hauptfeldwebel\"> /u/Hauptfeldwebel </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7d8q0/best_psu_for_low_power_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7d8q0/best_psu_for_low_power_nas/\">[comments]</a></span>",
        "id": 4506353,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7d8q0/best_psu_for_low_power_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best PSU for low power NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/thebloodreaper6739",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T14:35:11.428266+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T13:56:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Yea as title says, i wanna get my lectures and run it through local ai for note-taking, I&#39;ve tried stachy for the yt-dlp gui i think and it didnt work<br/> it was just able to download videos that did not need membership and threw an error once it reached the point of membership videos<br/> i gave it access to my cookies to allow it to login and it still didnt work<br/> any suggestions to work around this would be appreciated</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/thebloodreaper6739\"> /u/thebloodreaper6739 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7cfpb/need_help_downloading_lectures_in_youtube/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7cfpb/need_help_downloading_lectures_in_youtube/\">[comments]</a></span>",
        "id": 4506354,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7cfpb/need_help_downloading_lectures_in_youtube",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help downloading lectures in youtube membership that ive paid for and have access to",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Takssista",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T14:35:11.762073+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T13:44:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello fellow hoarders.</p> <p>I need to buy a large quantity of SD Cards (not micro). Anyone knows of a good supplier that supplies in bulk and possibly gives quantity discounts? EU area only, please.</p> <p>Cheers!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Takssista\"> /u/Takssista </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7c5j0/where_to_buy_sd_cards_in_bulk_eu_area/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q7c5j0/where_to_buy_sd_cards_in_bulk_eu_area/\">[comments]</a></span>",
        "id": 4506355,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7c5j0/where_to_buy_sd_cards_in_bulk_eu_area",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Where to buy SD Cards in bulk (EU area)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sindrealmost",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T11:14:02.727889+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T10:24:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p> <p>I am looking for recommendations on <strong>non-enterprise backup software</strong> that meets the following requirements. I am aware this may be roughly equivalent to asking whether anyone has a unicorn I could buy, but I am hoping it is merely rare rather than entirely mythological.</p> <ol> <li>Can perform scheduled backups</li> <li>Supports <strong>differential and incremental backups</strong></li> <li>Maintains a <strong>separate manifest or metadata file</strong>, such that the original full backup does not need to be present in order to generate a differential or incremental backup</li> </ol> <p>My current workflow involves creating a full backup, archiving it off-site on optical media, and then generating differential or incremental backups over time. The problem I am running into with most consumer and prosumer tools is that they require the entire full backup set to be locally available in order to produce subsequent backups, ",
        "id": 4504853,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q78bcl/question_backup_software_that_supports",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "[Question] Backup software that supports differentials when the full backup is offline",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kremsi2711",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T10:06:31.352529+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T09:19:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Do you have recommendations for 19\u201c U4 Storage chassis with at least 24 bays hot swap or top loaded?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Kremsi2711\"> /u/Kremsi2711 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q77a31/u4_case_recommendations/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q77a31/u4_case_recommendations/\">[comments]</a></span>",
        "id": 4504465,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q77a31/u4_case_recommendations",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "U4 Case recommendations",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Personal-Leather-177",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T10:06:31.458620+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T09:05:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Like whats the best temp is 50celsius okay </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Personal-Leather-177\"> /u/Personal-Leather-177 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q771dz/hey_how_do_i_know_whats_the_idle_temp_for_my/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q771dz/hey_how_do_i_know_whats_the_idle_temp_for_my/\">[comments]</a></span>",
        "id": 4504466,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q771dz/hey_how_do_i_know_whats_the_idle_temp_for_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Hey how do i know whats the idle temp for my Internal Hard Drive",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ottershavepockets",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T09:05:14.357877+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T08:34:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>SOLVED!!</p> <p>I have about 20 x 512gb M.2 sticks and am trying to find an easy solution to best use them. What would you all do solution wise? Are there any good, SFF NAS boxes or cradles etc? Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ottershavepockets\"> /u/ottershavepockets </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q76klr/solution_for_multiple_m2_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q76klr/solution_for_multiple_m2_drives/\">[comments]</a></span>",
        "id": 4504178,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q76klr/solution_for_multiple_m2_drives",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Solution for multiple M.2 drives",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Cultural_Acid",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T06:57:01.798671+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T05:53:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I didn&#39;t see anything about talking about web DL&#39;s here so this might get removed. I want to know the shows that are most likely to lose licenses and would be good to archive using DL tools. This is for preservation.</p> <p>(Also this has nothing to do with requesting downloads)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cultural_Acid\"> /u/Cultural_Acid </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q73ua3/what_shows_should_i_archive_from_streaming/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q73ua3/what_shows_should_i_archive_from_streaming/\">[comments]</a></span>",
        "id": 4503609,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q73ua3/what_shows_should_i_archive_from_streaming",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What shows should I archive from streaming?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CopiRightPlayer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T13:27:59.453750+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T05:07:15+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q72ypl/whats_the_best_way_to_rip_all_of_this_any/\"> <img src=\"https://a.thumbs.redditmedia.com/muUHxtGPhAHFWbyR-B1g-ueXwDzhF_-amQ9v52PBW50.jpg\" alt=\"What's the best way to rip all of this? Any shortcuts?\" title=\"What's the best way to rip all of this? Any shortcuts?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>TLDR: I am looking to figure out how to rip 2-2.5k discs, optimizing for speed rather then just seeking software automation. As well as getting a second opinion on the amount of storage I&#39;m gonna need.</p> <p>Context: I&#39;ve recently inherited my Mum&#39;s movie collection and decided to rip them, and at the same time rip my Dad&#39;s. I haven&#39;t ripped this amount of media before, and as it&#39;s probably best to put some thought into a dedicated setup of some kind, I&#39;ve decided to ask for advice. </p> <p>I am currently equipped with two LG WH14NS40 drives running LibreDrive 1.02 firmw",
        "id": 4505844,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q72ypl/whats_the_best_way_to_rip_all_of_this_any",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://a.thumbs.redditmedia.com/muUHxtGPhAHFWbyR-B1g-ueXwDzhF_-amQ9v52PBW50.jpg",
        "title": "What's the best way to rip all of this? Any shortcuts?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/librarian_at_789",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T03:42:55.688609+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T02:52:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Wikipedia is already swimming in cash and does not need that much money to stay afloat. Not a cent goes to the actual editors who edit the encyclopedia, but funds the WMF&#39;s bureaucracy, diversity programs, etc. See here: <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Fundraising_statistics\">https://en.wikipedia.org/wiki/Wikipedia:Fundraising_statistics</a></p> <p>Also see this essay (not written by me): <a href=\"https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_Cancer\">https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_Cancer</a></p> <p>(This is well-documented and there are plenty of sources on the topic. You can find them yourself.)</p> <p>On the other hand, the Internet Archive archives most of the Web (including Wikipedia and its deleted articles) without adding any editorial bias, as well as archiving other types of rare media such as classic software and music, and they face constant legal action from big publishers, for e",
        "id": 4502827,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q7053z/dont_donate_money_to_wikipedia_but_donate_your",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Don't donate money to Wikipedia, but donate your time to WP by editing, and donate your money to the Internet Archive (Wayback Machine) instead!",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vanceza",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T01:31:51.276958+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T01:04:12+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><ul> <li><a href=\"https://www.reddit.com/r/DataHoarder/comments/e3nb2r/longterm_reliability_testing/\">Year 0</a> - I filled 10 32-GB Kingston flash drives with pseudo-random data.</li> <li><a href=\"https://www.reddit.com/r/DataHoarder/comments/lwgsdr/research_flash_media_longevity_testing_1_year/\">Year 1</a> - Tested drive 1, zero bit rot. Re-wrote drive 1 with the same data.</li> <li><a href=\"https://www.reddit.com/r/DataHoarder/comments/tb26cy/flash_media_longevity_testing_2_years_later/\">Year 2</a> - Tested drive 2, zero bit rot. Re-tested drive 1, zero bit rot. Re-wrote drives 1-2 with the same data.</li> <li><a href=\"https://www.reddit.com/r/DataHoarder/comments/102razr/flash_media_longevity_testing_3_years_later/\">Year 3</a> - Tested drive 3, zero bit rot. Re-tested drives 1-2, zero bit rot. Re-wrote drives 1-3 with the same data.</li> <li><a href=\"https://www.reddit.com/r/DataHoarder/comments/18w3bxw/flash_media_longevity_testing_4_years_later/\">Y",
        "id": 4502226,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q6xnun/flash_media_longevity_testing_6_years_later",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Flash media longevity testing - 6 years later",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Shaymans_Origins",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T01:31:51.905723+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T00:47:59+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q6xa52/digitalizing_trough_capture_card/\"> <img src=\"https://b.thumbs.redditmedia.com/hnlYNbuVDrVlIdqTvXbSpnjR93u-GhwukbPsLR14GbQ.jpg\" alt=\"Digitalizing trough capture card\" title=\"Digitalizing trough capture card\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Heya, I&#39;m quite new to the av cables and capture cards, but I recently had new DVDs that didn&#39;t read trough my external DVD reader/burner. However the same discs did read using a portable DVD player (Panasonic dvd ls-58 out of my head). Got told to buy these cables and this card to digitalize the dvds trough the Panasonic dvd player. When I plugged all in, installed obs studio, I got sound, but a blue screen, pulled cleaned and plugged cables in (while pulling the cables out, I noticed sound right didn&#39;t appear to have input ( idk if that&#39;s the dvd or the cable). But the blue screen is something I&#39;m also annoyed by because I simply",
        "id": 4502227,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q6xa52/digitalizing_trough_capture_card",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/hnlYNbuVDrVlIdqTvXbSpnjR93u-GhwukbPsLR14GbQ.jpg",
        "title": "Digitalizing trough capture card",
        "vote": 0
    }
]