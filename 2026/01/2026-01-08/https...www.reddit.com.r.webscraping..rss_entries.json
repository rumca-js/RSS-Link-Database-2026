[
    {
        "age": null,
        "album": "",
        "author": "/u/OtherwiseGroup3162",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T15:57:41.713125+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T15:43:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have built a lot of different web scrapers in python that use HTTP requests and they work pretty well...</p> <p>However, we are now looking to scale and orchestrate a lot of them on an ongoing basis.</p> <p>What is the best way to monitor them and if one fails, see where the fail point is easily?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/OtherwiseGroup3162\"> /u/OtherwiseGroup3162 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q7f428/scaling_and_monitoring/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q7f428/scaling_and_monitoring/\">[comments]</a></span>",
        "id": 4507176,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q7f428/scaling_and_monitoring",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Scaling and Monitoring",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sardanioss",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T10:23:01.816689+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T08:24:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Most of the HTTP clients like requests in python gets easily flagged by Cloudflare and such. Specially when it comes to HTTP/3 there are almost no good libraries which has native spoofing like chrome. So I got a little frustated and had built this library in Golang. It mimics chrome from top to bottom in all protocols. This is still definitely not fully ready for production, need a lot of testing and still might have edge cases pending. But please do try this and let me know how it goes for you - <a href=\"https://github.com/sardanioss/httpcloak\">https://github.com/sardanioss/httpcloak</a> </p> <p>Thanks to cffi bindings, this library is available in Python, Golang, JS and C#</p> <p>It mimics Chrome across HTTP/1.1, HTTP/2, and HTTP/3 - matching JA4, Akamai hash, h3_hash, and ECH. Even does the TLS extension shuffling that Chrome does per-connection.. Won&#39;t help if they&#39;re checking JS execution or browser APIs - you&#39;d need a real browser fo",
        "id": 4504576,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q76eo1/built_an_http_client_that_matches_chromes",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 1,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Built an HTTP client that matches Chrome's JA4/Akamai fingerprint",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Acrobatic-Frame-5305",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T03:59:33.696244+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T02:19:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m working on a Python-based pipeline that ingests obituary text from multiple sources ( publishers, aggregators). The ingestion side is solid \u2014 the challenge is <strong>parsing consistency across highly variable, semi-structured text</strong>. This is <strong>not</strong> a pure ML problem \u2014 I\u2019m intentionally using deterministic / rule-based parsing first to avoid hallucination and compliance issues. Current stack is Python (regex, heuristics, text normalization), but I\u2019m open to light NLP if justified.</p> <p>If you\u2019ve dealt with:</p> <ul> <li>Messy human-written prose</li> <li>Semi-structured documents</li> <li>High precision &gt; recall requirements</li> <li>Real-world parsing (not academic NLP)</li> </ul> <p>\u2026I\u2019d love input, patterns, or even war stories.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Acrobatic-Frame-5305\"> /u/Acrobatic-Frame-5305 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscrap",
        "id": 4502883,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6zdzl/need_help_improving_text_parsing_python_nlplite",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need help improving text parsing (Python, NLP-lite, rule-based)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Lazy-Masterpiece8903",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-08T00:38:30.311103+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-08T00:25:59+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building a comparison tool for 7 sites currently just working on each scraper. BUT when it comes to comparing products across 7 sites what&#39;s going to be the best way to go with matching the products?</p> <p>Obviously fuzzy matching titles will be the most common solution. But could I use Ai to improve match rate or something? TIA</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Lazy-Masterpiece8903\"> /u/Lazy-Masterpiece8903 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6wrgy/matching_products_across_7_sites/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q6wrgy/matching_products_across_7_sites/\">[comments]</a></span>",
        "id": 4501948,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q6wrgy/matching_products_across_7_sites",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Matching Products Across 7 Sites",
        "vote": 0
    }
]