[
    {
        "age": null,
        "album": "",
        "author": "/u/abcsoups",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-09T23:57:12.959684+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-09T23:06:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I recently finished building a pipeline that continuously scrapes local events across multiple cities to power a live local-happenings map app. Wanted to share some techniques that worked well in case they&#39;re useful to others.</p> <p><strong>The challenge I found:</strong> Traditional event aggregators rely on manual submissions. I wanted to autonomously discover events that don&#39;t get listed elsewhere, often get missed - neighborhood trivia nights, recurring happy hours, small business live music, pop-up markets, etc.</p> <p> </p> <p><strong>My approach:</strong></p> <ul> <li><strong>Intelligent search</strong>: 30+ curated query templates per city (generic, categorical, temporal) pumped thru a variety of existing html-text-extracting Search APIs</li> <li><strong>LLM extraction</strong>: GPT-4o-mini (mainly, plenty strong enough) with context injection and heavily guardrailed structured output instructions + Pydantic validation</li> <li><stron",
        "id": 4520645,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q8ndvm/built_an_autonomous_event_discovery_pipeline",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Built an autonomous event discovery pipeline - crawl + layered LLMs",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Daeky03",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-09T23:57:13.125614+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-09T22:24:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>There&#39;s a Cloudflare block that only accepts requests from certain countries, and my machines can&#39;t access them. Free proxies in that location either stop working after 30-40 seconds or don&#39;t work at all Does anyone have a solution to suggest for this? I&#39;m using Node.js as my backend in my system. I can send you the URL via DM for you to try.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Daeky03\"> /u/Daeky03 </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q8mc1v/a_private_site_with_cloudflare_locationbased/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q8mc1v/a_private_site_with_cloudflare_locationbased/\">[comments]</a></span>",
        "id": 4520646,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q8mc1v/a_private_site_with_cloudflare_locationbased",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A private site with Cloudflare location-based restrictions.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/irungalur",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-09T15:37:52.478394+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-09T15:19:55+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am trying to parse the US house trade filings. You can download the yearly index file and get the doc id to get the actual filing pdf. In the index file what filing types represent stock/option transactions? The house website doesn&#39;t seem to have any documentation on this. Here are some of the filing types I got from the index files.</p> <p>&lt;FilingType&gt;A&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;B&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;C&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;D&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;E&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;G&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;H&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;O&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;P&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;T&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;W&lt;/FilingType&gt;</p> <p>&lt;FilingType&gt;X&lt;/FilingType&gt;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.",
        "id": 4516698,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q8b0j4/us_house_trade_index_file_filing_type",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "US House Trade Index file Filing Type",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Foxocommando",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-09T09:44:40.030308+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-09T09:24:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m looking for a SERP API that i cna use to get qualitative info about a prospect like any podcasts they appeared on, any blogs they\u2019ve mentioned their pain points in. I\u2019d also like to find more details about their products from their website, run it through ChatGPT and personalize my emails based on their specific product feature which my product can support.</p> <p>What are the best tools you\u2019ve used for getting this info? Thanks in advance guys!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Foxocommando\"> /u/Foxocommando </a> <br/> <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q83stp/best_serp_api_tool_for_b2b_data_collection/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/webscraping/comments/1q83stp/best_serp_api_tool_for_b2b_data_collection/\">[comments]</a></span>",
        "id": 4514146,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q83stp/best_serp_api_tool_for_b2b_data_collection",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Serp API tool for B2B data collection",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Azuriteh",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-09T09:44:40.228965+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-09T08:52:35+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/webscraping/comments/1q83adc/presenting_tlshttp_a_tlsclient_wrapper_from_go/\"> <img src=\"https://external-preview.redd.it/LaDWLnUkZEgBUokn6pdghoDm9p0gKtoL8ClV-7i7Cvo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9db9ef9d4446fd49c8648b2579e1a42216e9d311\" alt=\"Presenting tlshttp, a tls-client wrapper from Go\" title=\"Presenting tlshttp, a tls-client wrapper from Go\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Yes, I know there&#39;s tls-client for Python already, following the requests syntax, but it&#39;s outdated and I prefer httpx syntax! So I decided to create my own wrapper: <a href=\"https://github.com/Sekinal/tlshttp\">https://github.com/Sekinal/tlshttp</a></p> <p>I&#39;m already using it on some private projects!</p> <p>If you&#39;ve got absolutely no idea on what this is used for, it&#39;s to spoof your requests to not make it as obvious you&#39;re scraping a given API!, bypassing basic bot protection.</p> </div><!-- SC",
        "id": 4514147,
        "language": "en",
        "link": "https://www.reddit.com/r/webscraping/comments/1q83adc/presenting_tlshttp_a_tlsclient_wrapper_from_go",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 467,
        "source_url": "https://www.reddit.com/r/webscraping/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/LaDWLnUkZEgBUokn6pdghoDm9p0gKtoL8ClV-7i7Cvo.png?width=640&crop=smart&auto=webp&s=9db9ef9d4446fd49c8648b2579e1a42216e9d311",
        "title": "Presenting tlshttp, a tls-client wrapper from Go",
        "vote": 0
    }
]