[
    {
        "age": null,
        "album": "",
        "author": "/u/wiseguyz23",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:51.927373+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T23:24:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey Everyone,</p> <p>I have a 3tb WDD 3.5&quot; Blue drive I&#39;ve had for an archive drive for 3 computers now. It&#39;s a tank and has been my constant companion for almost 8 years. When I got a NAS (Q-NAP TS230) I threw 2x3TB WD RED drives into it in RAID 1.</p> <p>I threw my Archive drive onto the NAS (AS IS) in a folder named the same as my drive. Well, along the way I change things around on my 3TB WD in my PC and organized everything. I deleted a bunch of files, etc. I ALSO threw more stuff onto the NAS to clear space on the archive drive. </p> <p>I realized my mistake when I went to move to my new computer and bring my 3TB WD with me and wanted to transfer my data from my old PC to my new PC through my NAS. Everything was completely different. </p> <p>So, now I have files ALL over the place in both drives. So, here are my needs and facts if anyone wants to help a guy out. :) </p> <ol> <li>I have files on the NAS that aren&#39;t on my archive ",
        "id": 4473069,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q453e2/need_help_w_nas_archive_drive_out_of_sync",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Need Help w/ NAS + Archive Drive Out Of Sync",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sekiroborne",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:51.385282+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T22:58:11+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q44f9o/is_this_a_good_deal_if_i_dont_want_barracuda_for/\"> <img src=\"https://preview.redd.it/6eamulanxebg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c653d7f52f980472b5c33c1eb1423f6d818247c\" alt=\"Is this a good deal if I don\u2019t want Barracuda for NAS?\" title=\"Is this a good deal if I don\u2019t want Barracuda for NAS?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Found this WD red pro for $440 (at Micro Center). Is this more reliable than the Barracuda and does it worth the price difference? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sekiroborne\"> /u/sekiroborne </a> <br/> <span><a href=\"https://i.redd.it/6eamulanxebg1.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q44f9o/is_this_a_good_deal_if_i_dont_want_barracuda_for/\">[comments]</a></span> </td></tr></table>",
        "id": 4473068,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q44f9o/is_this_a_good_deal_if_i_dont_want_barracuda_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/6eamulanxebg1.jpeg?width=640&crop=smart&auto=webp&s=2c653d7f52f980472b5c33c1eb1423f6d818247c",
        "title": "Is this a good deal if I don\u2019t want Barracuda for NAS?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/user1138713",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:52.214656+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T22:07:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am wondering if I&#39;m the only one with this problem but the search bar gives incomplete and inconsistent result for almost all websites (for instance when searching for &quot;x.com/*&quot; I only get a small amount of the saves made at a given time, most of the one I made rarely appear if I don&#39;t give the complete URL). Am I using it wrong or is the tool just not working for everyone ?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/user1138713\"> /u/user1138713 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q4357l/archivephtoday_search_is_broken/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q4357l/archivephtoday_search_is_broken/\">[comments]</a></span>",
        "id": 4473070,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q4357l/archivephtoday_search_is_broken",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "archive.ph/today search is broken ?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/byooni",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T22:47:28.125522+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T21:59:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m planning on buying an external storage mainly to store years of photos, videos, and memories. I&#39;m leaning towards HDD because my father&#39;s external HDD has been in a house fire yet the data inside was successfully saved (I don&#39;t really know if he got the disk placed into a new device or the data was moved into a brand new HDD. I was only 10 back then), which makes me believe HDD is a safer option. But I know so little about SSDs so I thought it would be better to ask which one is the safest when it comes to saving the data in case of a similar accident or the risk of the data getting corrupted from unforseen circumstances.</p> <p>p.s. I don&#39;t really use Reddit so forgive me if I don&#39;t reply to your comments for days or even weeks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/byooni\"> /u/byooni </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q42xfz/which_type_",
        "id": 4472716,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q42xfz/which_type_of_external_storage_should_i_buy",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which type of external storage should I buy?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/TattooedBrogrammer",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T22:47:28.240747+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T21:17:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Outgrown my current desktop solution and need more Sata ports. Looking at adding my first LSI Raid Card, ideally a 9400-16i. Was looking around in Canada but new they only stock 9500s and those things run 1k+ CAD from authorized resellers, while Ebay has 9400s \u201cnew\u201d for 300 CAD from China. My question is, knowing they are likely counterfeit goods, how bad are they performance and reliability wise? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TattooedBrogrammer\"> /u/TattooedBrogrammer </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q41xnd/how_bad_are_counterfeit_raid_cards/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q41xnd/how_bad_are_counterfeit_raid_cards/\">[comments]</a></span>",
        "id": 4472717,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q41xnd/how_bad_are_counterfeit_raid_cards",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How bad are counterfeit raid cards?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bttech05",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T21:17:07.776692+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T20:52:31+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been through several methods over the years. Extreme picture finder used to work, but now it only downloads photos and not videos. I\u2019ve also seen several bash scripts that apparently have worked for some but not others.</p> <p>Does anyone have a method thats working for them?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bttech05\"> /u/bttech05 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q41afb/downloading_from_leakedzone/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q41afb/downloading_from_leakedzone/\">[comments]</a></span>",
        "id": 4472303,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q41afb/downloading_from_leakedzone",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Downloading from LeakedZone",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/JakeVGN",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:52.471489+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T20:45:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019m a musician and designer/artist so I have images I\u2019ve designed, videos and sound recordings crammed and spread over a few devices that I\u2019d like to keep in one place. I\u2019m sick of taking my phone out with all the data i dont need at that time on it and it not working cause it\u2019s full. Ive look into external hard drives but I\u2019m not that tech savvy to know what\u2019s right as it say some only last a few years and others have different downfalls like needing to be plugged in constantly. I don\u2019t know why I can\u2019t get a That\u2019s just like an iPad or something where I go in and get an art project I wanna work on then save it to that device. But have loads of storage too and not have it shatterable like an iPad is with its screen which has happened to me before. How do other people store all their projects, etc.?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/JakeVGN\"> /u/JakeVGN </a> <br/> <span><a href=\"https://www.reddit.c",
        "id": 4473071,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q413gc/permanent_external_storage_solutions",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Permanent External Storage solutions",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/c8d85",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T21:17:07.932721+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T20:32:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am generally a bit hesitant to re-encode because it can only reduce quality, but .webm is probably the exception. </p> <p>Do you all re-encode webm files or should I just open them in VLC and not think about it again? If so, to what format/codec?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/c8d85\"> /u/c8d85 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q40rfr/what_do_you_do_with_your_webm_files/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q40rfr/what_do_you_do_with_your_webm_files/\">[comments]</a></span>",
        "id": 4472304,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q40rfr/what_do_you_do_with_your_webm_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What do you do with your .webm files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Alexschmidt711",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T20:06:14.712760+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T19:32:42+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3z65a/possible_concerning_signs_for_livejournals/\"> <img src=\"https://external-preview.redd.it/v2kPBfSB7JqjDrv31iQOQLnX9l9cFBy3BHz0cwJ79CM.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f64ca2b83cc26ac29801de6d93698be9b4f1a1d\" alt=\"Possible concerning signs for LiveJournal's survival outside Russia in response to sanctions there?\" title=\"Possible concerning signs for LiveJournal's survival outside Russia in response to sanctions there?\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Alexschmidt711\"> /u/Alexschmidt711 </a> <br/> <span><a href=\"https://bsky.app/profile/rahaeli.bsky.social/post/3mbebi2xfxc25\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3z65a/possible_concerning_signs_for_livejournals/\">[comments]</a></span> </td></tr></table>",
        "id": 4471980,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3z65a/possible_concerning_signs_for_livejournals",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/v2kPBfSB7JqjDrv31iQOQLnX9l9cFBy3BHz0cwJ79CM.jpeg?width=640&crop=smart&auto=webp&s=0f64ca2b83cc26ac29801de6d93698be9b4f1a1d",
        "title": "Possible concerning signs for LiveJournal's survival outside Russia in response to sanctions there?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/avatar_94",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T20:06:15.549994+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T19:24:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Sorry if I&#39;m not meant to post this here, I uninstalled DVD fab through the windows 10 settings and it pretty much wiped the whole hard drive clean of movies, is there any way to revert those changes. </p> <p>Litterally just happend, any help is welcome</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/avatar_94\"> /u/avatar_94 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3yyac/i_uninstalled_dvd_fab_and_it_deleted_a_lot_of_mkv/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3yyac/i_uninstalled_dvd_fab_and_it_deleted_a_lot_of_mkv/\">[comments]</a></span>",
        "id": 4471982,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3yyac/i_uninstalled_dvd_fab_and_it_deleted_a_lot_of_mkv",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I uninstalled DVD Fab and it deleted a lot of mkv movie files",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Stan_Stanman",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T20:06:14.857622+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T19:22:44+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a 3.5&quot; floppy disk drive attached by USB cable to a Windows 11 machine. I have a large archive of software on 3.5&quot; disks from the 90s. Some are for multi-disk installations. Ideally, I would rip each one to an ISO and then have another tool that would allow me to mount it virtually whenever needed. Would that be the recommended best practice? If so, is there a defacto standard software I should use? I had looked into PowerISO and WinImage but both seem to contain spyware or other elements that trigger my virus/malware software. Thanks! </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Stan_Stanman\"> /u/Stan_Stanman </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3ywbc/best_practices_for_archiving_35_floppy_disks/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3ywbc/best_practices_for_archiving_35_floppy_disks/\">[comments]</a></span",
        "id": 4471981,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3ywbc/best_practices_for_archiving_35_floppy_disks",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Best Practices for Archiving 3.5\" Floppy Disks",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/vintagedragon9",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T19:00:45.303373+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T18:43:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So, for Christmas my husband and I made the mistake of getting unionsine external hard drives from Newegg. Mostly my mistake, as I didn&#39;t research the brand. If I had I would&#39;ve learned they (Unionsine) have a history of selling old hard drives as new ones. These want to randomly disconnect and I get the &quot;there is a problem with this device&quot; pop-up frequently. </p> <p>Now, I&#39;m looking into a replacement and saw the 2TB Seagate sthn2000400. I decided to do some research this time and read that brand isn&#39;t the only factor, but model as well. As even big-name brands have some models that are duds. To sum it up, is the sthn2000400 any good? I&#39;m not looking for top-of-the-line. I&#39;m looking for &quot;does its job without major issues.&quot; I want to avoid the issues I had with the Unionsine shit brick. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vintagedragon9\"> /u/vintagedragon9",
        "id": 4471639,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3xulc/how_is_the_seagate_sthn2000400",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How is the Seagate sthn2000400?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wahugg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T17:55:09.702409+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T17:30:19+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3vvhv/my_fight_w_windows_storage_spaces/\"> <img src=\"https://b.thumbs.redditmedia.com/v3F1EkCH_ETwur5Pw_5flsmwRV4jvpRi57_4SIVGfvM.jpg\" alt=\"My Fight w/ Windows Storage Spaces\" title=\"My Fight w/ Windows Storage Spaces\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Like many others, I am experiencing slow write speeds with Windows Storage Spaces despite research and trial &amp; error on how to properly setup relationships between the number of columns, AUS, and interleave.</p> <p>System/ Context:</p> <ul> <li>My hypervisor is Windows Server 2022 Datacenter and I cannot change this</li> <li>5x 4TB Seagate Ironwolf CMR drives</li> <li>4 of the drives connect natively to the motherboard, 1 is through a PCIe -&gt; SATA expansion card</li> <li>I used the following articles as guides: <ul> <li><a href=\"https://www.reddit.com/r/DataHoarder/comments/179xc1g/my_storage_spaces_experience/\">My Storage Spaces Experienc",
        "id": 4471192,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3vvhv/my_fight_w_windows_storage_spaces",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/v3F1EkCH_ETwur5Pw_5flsmwRV4jvpRi57_4SIVGfvM.jpg",
        "title": "My Fight w/ Windows Storage Spaces",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Jazzlike-Drawing-644",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T16:49:03.526101+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T16:47:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Trying to burn some DVDs isos I have found online. They play fine in VLC but when I try to burn them via imgburn, dvddecrypter, built in Windows, I get a DVD that has black and green covering the video. \u200b\u200bMake mkv works and I can get the mkv files and split them accordingly into episodes but I truly just want to burn these into discs to keep and watch on the old crt later on. Any tips would help me immensely! Thanks! \u200b\u200b</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jazzlike-Drawing-644\"> /u/Jazzlike-Drawing-644 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3upvk/garbled_green_black_mess_burning_iso_dvd/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3upvk/garbled_green_black_mess_burning_iso_dvd/\">[comments]</a></span>",
        "id": 4470816,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3upvk/garbled_green_black_mess_burning_iso_dvd",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Garbled green black mess burning iso dvd",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Allmotr",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T16:49:03.661043+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T16:31:42+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I ordered 4 20tb red pro\u2019s for a custom NAS build with a intel 14500,Tuf B760, 32gb ram, Seasonic prime titanium 850w. Some new, some used parts.</p> <p>Built the pc yesterday, install the hardrives to the sata ports on the motherboard. PC won\u2019t even start.</p> <p>Here\u2019s where it gets weird, when i go to start the PC , the PSU fan turns on, i hear a click then Psu fan stops spinning indicating a short. Nothing else turns on.</p> <p>Start diagnosing the PSU as i got it from ebay even though it looked brand new. Paperclip test etc, as soon as i remove the sata cable pigtail from psu the computer starts. This is my first nas build and i have all 4 HDDs on 1 pigtail, i realize i cant do that and i get another sata pigtail and plug 2 HDDs to each one. Still wont start.</p> <p>So now i check sata cables, they are all plugged in, unplug all but 1, pc starts. So i check each one and notice it only happens with 1 cable, i replace the sata cable and it starts.<",
        "id": 4470817,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3ubhj/what_are_the_chances_of_receiving_3_new_doa_20tb",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What are the chances of receiving 3 new DOA 20tb WD Red pro?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/BarberPlane3020",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:52.796073+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T15:48:00+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just read about this new 32TB HDD. More info there -&gt; <a href=\"https://www.techradar.com/pro/unannounced-32tb-seagate-hard-drive-surfaces-in-japan-for-photoshoot-ironwolf-pro-gets-a-shockingly-high-usd887-price-tag\">https://www.techradar.com/pro/unannounced-32tb-seagate-hard-drive-surfaces-in-japan-for-photoshoot-ironwolf-pro-gets-a-shockingly-high-usd887-price-tag</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/BarberPlane3020\"> /u/BarberPlane3020 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3t6v3/seagate_32tb_st32000nt000_ironwolf_is_for_sale_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3t6v3/seagate_32tb_st32000nt000_ironwolf_is_for_sale_in/\">[comments]</a></span>",
        "id": 4473072,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3t6v3/seagate_32tb_st32000nt000_ironwolf_is_for_sale_in",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Seagate 32TB ST32000NT000 IronWolf is for sale in Japan for $887",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Solderking",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T15:42:03.795760+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T15:28:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>My NAS is a raspberry 5 running OpenMediaVault 7, connected to a DAS enclosure.</p> <p>I recently had an overheating issue (fan failure) where the drives got up to 80C for about 1-2 hours. The drives are now in the 30-40 range with a repaired fan, and I&#39;m able to read and write data without issue, but I thought I should check on my drives.</p> <p>I have two seperate raid1 mdadm arrays. md120 consists of 2 640GB WD drives, and md121 consists of 2 10TB WD drives. When I run mdadm --detail /dev/mdX, I get details but I&#39;m not sure how to interpret it. One says it only has 1 RAID device, and 1 says it has 2 RAID devices. One says it is degraded, but the other says it is clean. I&#39;m not really sure what these means or what I should do about it.</p> <p>terminal output of mdadm --detail /dev/mdX:</p> <p><a href=\"https://pastebin.com/ZgR21guf\">https://pastebin.com/ZgR21guf</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.re",
        "id": 4470407,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3sp61/nas_drives_mdadm_degraded_are_my_drives_bad",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NAS drives - mdadm degraded, are my drives bad?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Anxious_Noise_8805",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T15:42:04.103824+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T15:21:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p><strong>**Note to Mods: Although this platform is cryptocurrency related, it is relevant to data hoarders because it involves hoarding and sharing video cam content of which there are many people in this sub*\\</strong>*</p> <p>Our white paper is now complete (link below)! Key features of the upcoming CaptureGem content trading platform:</p> <p><strong>Trust-Based Delivery</strong>: Payments are held in escrow and only released to IPFS storage providers (Peers) once the purchaser&#39;s client confirms successful content delivery. This ensures infrastructure providers are rewarded based on actual performance.</p> <p><strong>Collection Tokens (Token-2022)</strong>: Every content library has a unique token with an 80/10/10 distribution: 80% to Orca liquidity pools, 10% to creator wallet (person who initially records the content), and 10% to a Claim Vault for IP protection.</p> <p><strong>Dividends to collection token holders</strong>: 50% of all collectio",
        "id": 4470408,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3sitc/new_white_paper_released_for_the_upcoming",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "New white paper released for the upcoming CaptureGem decentralized cam video content trading platform built on Solana",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/bluewatercat",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T15:42:04.412074+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T15:07:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have several old photo albums that I need to digitize, but I will also be digitizing old cookbooks and other similar books from my grandmother, so I am looking at a flatbed scanner. My budget is about $200, though there is a bit of wiggle room if the extra cost is worth it.</p> <p>I&#39;ve been scouring the various threads for input on a photo scanner, but it seems the overwhelming consensus is for the Epson 680, which is really out of budget, and I&#39;ve also seen suggestions for a DSLR/camera stand setup, but I don&#39;t think that would work for some of the books I need scanned either. </p> <p>I have been looking at the Canon CanoScan Lide 400, though there are some iffy reviews on it regarding the compatibility of drivers with newer OSs and getting &quot;lemons&quot;. Another one I&#39;ve seen is the Epson Perfection V19, which seems similar to the Canon. </p> <p>Does anyone have any suggestions or comments on something that could work for what",
        "id": 4470409,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3s6uu/flatbed_photo_scanner_recommendation_budget_200",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Flatbed Photo Scanner Recommendation, Budget $200 with Wiggle Room",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/louigi_verona",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T14:35:10.984609+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T14:20:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Folks!</p> <p>This is the first time I stumbled into an SMR drive. In fact, before today I have never heard about this &quot;technology&quot;. It takes hours to copy even small files. It seems to struggle even copying 1Mb mp3s. Some folders seem to be processed fast, some take hours. And the write speed becomes slower and slower into the process.</p> <p>I am thinking of returning this drive and buying an CMR one instead. But given that the manufacturers are not labeling SMR drives, can anyone suggest a drive that&#39;s confirmed to be CMR?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/louigi_verona\"> /u/louigi_verona </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3r3bi/got_an_external_2tb_seagate_drive_turned_out_smr/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3r3bi/got_an_external_2tb_seagate_drive_turned_out_smr/\">[comments]</a></span>",
        "id": 4470012,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3r3bi/got_an_external_2tb_seagate_drive_turned_out_smr",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Got an external 2TB Seagate drive, turned out SMR. How to pick CMR instead?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Herbert1627",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:53.390069+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T14:13:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey I\u2019m currently hording all my data on a Nas through the Router it\u2019s good but I thought let\u2019s buy a proper Nas so I did at least I think.</p> <p>My problem with the drives attached to the Router directly I get like 100 MBps of speed but with the Synology DS214+ only up to 20 MBps.</p> <p>Does anyone had this problem or know how to get more speed</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Herbert1627\"> /u/Herbert1627 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3qx37/synology_ds214/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3qx37/synology_ds214/\">[comments]</a></span>",
        "id": 4473074,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3qx37/synology_ds214",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Synology DS214+",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sufficient-Year4640",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T23:52:53.073542+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T13:03:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When someone puts in a paywalled url and attempts to archive it, how is <a href=\"http://archive.ph\">archive.ph</a> able to extract (some of) the the underlying data? (I&#39;m aware it can extract only certain types of data - like text, not video etc).</p> <p>Why can&#39;t the paywalled website just block archive.ph?</p> <p>According to claude</p> <blockquote> <p>When you submit a URL to <a href=\"http://archive.ph\">archive.ph</a>, <strong>your browser</strong> does the work. You&#39;re already viewing the page (maybe you&#39;re a subscriber, maybe you got a free view, whatever). Archive.ph&#39;s interface grabs the rendered content from <strong>your browser</strong> and uploads it to their servers.</p> <p>It&#39;s not archive.ph going out and fetching the page from the newspaper. It&#39;s you \u2014 sitting there with the article already loaded \u2014 sending what&#39;s on your screen to archive.ph.</p> <p>That&#39;s why it doesn&#39;t matter if the newspaper bl",
        "id": 4473073,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3pfzy/how_is_archiveph_able_to_access_the_underlying",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How is archive.ph able to access the underlying data of a paywalled page",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/brijazz012",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T13:31:21.562982+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T12:59:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have been running BDFR without issue for years. I recently enabled 2FA on my account, so it stands to reason that I&#39;ll need to reauthenticate. I&#39;m running this:</p> <blockquote> <p>bdfr download ~/Downloads/BulkRedditDownloader --user me --saved --authenticate</p> </blockquote> <p>and getting this error:</p> <p><code>AttributeError: module &#39;praw.reddit&#39; has no attribute &#39;BaseTokenManager&#39;</code></p> <p>I had expected BDFR to provide a URL for authentication as I believe it has done in the past. Any suggestions on how to reauthenticate?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/brijazz012\"> /u/brijazz012 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3pd25/bdfr_400_http_response/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3pd25/bdfr_400_http_response/\">[comments]</a></span>",
        "id": 4469703,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3pd25/bdfr_400_http_response",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "BDFR - 400 HTTP response",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jorvaor",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T13:31:22.037012+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T12:54:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I was making research before buying a microSD card and found a website that would be of interest for this subreddit. It is about a madlad that has been benchmarking microSD cards for two years.</p> <blockquote> <p>Think of it as the answer to such questions as \u201cjust how good are microSD cards that you buy off of AliExpress?\u201d, or \u201cjust how long should I expect an SD card to last?\u201d, or \u201cwhat\u2019s the best microSD card you can get for under $15?\u201d, or \u201cwhy did Matt spend SO MUCH MONEY on microSD cards just to destroy them??\u201d</p> </blockquote> <p>This is the site: <a href=\"https://www.bahjeez.com/\">https://www.bahjeez.com/</a></p> <p>Did I personally find anything useful? Yes, I found that Kingston brand is good enough for my use case (32GB card for an mp3 player).</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jorvaor\"> /u/jorvaor </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3p9mw/the_great",
        "id": 4469704,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3p9mw/the_great_microsd_card_survey",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "The Great MicroSD Card Survey",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/killianmcc",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T11:21:25.955105+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T11:01:08+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m going to have to bite the bullet and buy a new 12TB drive for my Unraid NAS server. I&#39;m between two models right now, both roughly the same price. Which should I go for?</p> <ul> <li>Seagate 12TB SATA 7.2k 3.5&quot; 6G Recertified HDD ST12000NM001G</li> <li>WD WD120EFGX Red Plus 12TB SATA Hard Drive for NAS w/ 7200RPM 6Gb/s 512MB Cache</li> </ul> <p>I&#39;m leaning WD as it&#39;s new rather than refurb and also 3 year warranty rather than 1 with the Seagate. Thoughts?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/killianmcc\"> /u/killianmcc </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3n8t3/which_drive_should_i_go_for_unraid_nas/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3n8t3/which_drive_should_i_go_for_unraid_nas/\">[comments]</a></span>",
        "id": 4469089,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3n8t3/which_drive_should_i_go_for_unraid_nas",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Which Drive Should I Go for? - Unraid NAS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/war_egg_burrito",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T11:21:25.475734+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T10:27:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a rar file that took TWELVE HOURS TO DOWNLOAD at 11 hours I realised I forgot to setup my ethernet and when I did that broke the download \ud83d\ude2d anyways the download was averaging 1.8 mb/s on WiFi and when I redownloaded it on ethernet it only took two hours averaging 11.1mb/s. It was a 67gb download. Sorry for waffling, but the question is what is the fastest way to extract it? Winrar, 7zip, or any other methods? I have a terrible build (intel i7 3770, 32gb ddr3 ram, no graphics card, etc) and as you can tell from above shitty internet, so what&#39;s the fastest way for me </p> <p>Edit: holy shit 7 zip did it in SIX MINUTES. For the record, winrar once took 7 hours for a 9gb rar file, which is why I thought this would take so long</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/war_egg_burrito\"> /u/war_egg_burrito </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3mois/quickest_way_to_e",
        "id": 4469088,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3mois/quickest_way_to_extract_rar_files",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Quickest way to extract rar files?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Exact-Contact-3837",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T10:19:16.266906+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T09:46:12+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3lzn6/i_built_a_selfhealing_storage_engine_in_rust/\"> <img src=\"https://external-preview.redd.it/NJtZrFR9AYJ3zT8lo-uP7vnJL1XiihOIM-dKWkLcES8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=52d12fce9580f3be0fb21079c0c4497f4e0a703b\" alt=\"I built a self-healing storage engine in Rust (Reed-Solomon) that mounts as a local drive.\" title=\"I built a self-healing storage engine in Rust (Reed-Solomon) that mounts as a local drive.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hi guys, I&#39;m very proud to introduce <strong>BlockFrame</strong>.</p> <p>BlockFrame is a self-healing storage engine built in Rust. My motivation to create this was personal experience with S3 and Databricks, specifically, the overwhelming sense of &quot;API fatigue&quot; trying to work with them.</p> <p>To be clear: This doesn&#39;t replace those services for enterprise scale. However, BlockFrame makes the medicine go down a bit easier ",
        "id": 4468860,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3lzn6/i_built_a_selfhealing_storage_engine_in_rust",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/NJtZrFR9AYJ3zT8lo-uP7vnJL1XiihOIM-dKWkLcES8.png?width=640&crop=smart&auto=webp&s=52d12fce9580f3be0fb21079c0c4497f4e0a703b",
        "title": "I built a self-healing storage engine in Rust (Reed-Solomon) that mounts as a local drive.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/CollarFlat6949",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T07:07:02.662521+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T06:22:56+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I want to download these images at their full size, but the archive website is using Islandora image viewer, and only downloads a tiny 50kb thumbnail. When I click in on the image viewer, I can see they are showing me a very high resolution image, but I can&#39;t figure out how to get it. I found a similar question on this sub from three years ago, but the solution there didn&#39;t work for me in this case. Anyone have any ideas?</p> <p>Link: <a href=\"https://www.louisianadigitallibrary.org/islandora/object/lsm-ccc%3A639\">https://www.louisianadigitallibrary.org/islandora/object/lsm-ccc%3A639</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CollarFlat6949\"> /u/CollarFlat6949 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3ikrq/any_idea_how_i_can_get_these_images/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3ikrq/any_idea_how_i_can_get_these_",
        "id": 4468220,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3ikrq/any_idea_how_i_can_get_these_images",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Any idea how I can get these images?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chumpy__",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T06:02:26.848369+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T05:43:30+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3hujw/terribly_slow_samsung_t7/\"> <img src=\"https://b.thumbs.redditmedia.com/Tr5mk2xWaD31o34GixlcNZozEPefv9tG85Qmm_OuRBA.jpg\" alt=\"Terribly Slow Samsung T7\" title=\"Terribly Slow Samsung T7\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have a Samsung T7 2TB external SSD fairly new (bought 12 months ago), I started transferring my GoPro 12 snowboard footage (4K60) but its either extremely slow or stuck at 0MB/S for 10-30mins.</p> <p>Sometimes at the very start of the transfer, it&#39;ll transfer at 500MB-1GB/S for 1 second then immediately drop to 0MB/S and freeze for 10-30mins.</p> <p>Why is it always going to 0MB/S? Its frustrating and slow, this has been a consistent problem ever since I got it, I&#39;m only posting this now because I&#39;m finally using it again after months to transfer footage.</p> <p>None of the files I have are corrupt or have issues. I&#39;m able to transfer 100GB+ of GoPro footag",
        "id": 4468030,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3hujw/terribly_slow_samsung_t7",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/Tr5mk2xWaD31o34GixlcNZozEPefv9tG85Qmm_OuRBA.jpg",
        "title": "Terribly Slow Samsung T7",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Afraid_Clothes2516",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T06:02:26.057590+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T05:02:13+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey y&#39;all! As the title mentions, I&#39;m looking to get into backing up my pc, but I have a fw questions and mild concerns.</p> <p>First is Image vs clone - I think I understand that image is something you can restore to a different drive in case of something going wrong, while a clone is a forced plug and play with the new disk vs the old. (If I am wrong, please feel free to correct)</p> <p>Now, my concern for this would be that my pc has multiple SSDs, so would this still work? Would it have two separate images, or is the clone software able to combine the two drives into one image while keeping the respective data inside their own drives? If that didn&#39;t make sense, I am sorry. What I mean is if I have, say, a game installed onto drive D, and some other stuff on drive C., would it make two separate images, one for disk C and one for D, which I restore individually, or would it image them into one file and restore exactly as they were before",
        "id": 4468029,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3h1ct/looking_to_possibly_get_into_backing_up_my_pc",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking to possibly get into backing up my pc (Guilty of not doing it before) but could use some help",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Maxray2",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T05:00:04.608865+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T04:39:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>So i have a desktop with about 10TB of space, most of which is used to store my downloaded content. I download via my laptop, and then transfer it to the desktop.</p> <p>Both are connected to the same wifi..but the speed of transfer comes to about 1-1.5 MBps. It takes a long time to transfer stuff. Both are windows operated. I access the desktop folders via the network icon, and then use teracopy to transfer the files.</p> <p>Can you please help me out? what can i do to increase the transfer speed? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Maxray2\"> /u/Maxray2 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3gklw/small_scale_hoarder_help_required_in_data_transfer/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3gklw/small_scale_hoarder_help_required_in_data_transfer/\">[comments]</a></span>",
        "id": 4467827,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3gklw/small_scale_hoarder_help_required_in_data_transfer",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Small scale hoarder - help required in data transfer",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/marhensa",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T03:55:25.208073+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T03:17:28+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3euse/open_source_ive_been_working_on_gui_for/\"> <img src=\"https://b.thumbs.redditmedia.com/IV2Gsfz3KqYguhJONb8tLxyGwzti6Lchcaftyhij9mE.jpg\" alt=\"[Open Source] I've been working on GUI for Instaloader to make downloading Instagram content easier (Portable Exe and AppImage)\" title=\"[Open Source] I've been working on GUI for Instaloader to make downloading Instagram content easier (Portable Exe and AppImage)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hey everyone, I&#39;ve been working on a desktop GUI wrapper for <strong>Instaloader</strong> to make it a bit easier to use for those who prefer an interface over the command line.</p> <p>It&#39;s open source and free. Just sharing in case anyone finds it useful for archiving their data.</p> <p>It&#39;s built with <strong>Python 3.10+</strong> and <strong>PyQt6</strong>, featuring a modern dark theme.</p> <p><strong>It&#39;s Portable</strong>, available as ",
        "id": 4467665,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3euse/open_source_ive_been_working_on_gui_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/IV2Gsfz3KqYguhJONb8tLxyGwzti6Lchcaftyhij9mE.jpg",
        "title": "[Open Source] I've been working on GUI for Instaloader to make downloading Instagram content easier (Portable Exe and AppImage)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dinoman1214",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T01:48:53.880215+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T01:42:53+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3crk3/im_going_to_film_school_later_this_year_and_i/\"> <img src=\"https://preview.redd.it/yzhtwqdkl8bg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=088af9711331667841680efa8a368c4bbbb50cbd\" alt=\"I\u2019m going to film school later this year and I need an external SSD for large video files. Is this a good option? Or should I choose something different?\" title=\"I\u2019m going to film school later this year and I need an external SSD for large video files. Is this a good option? Or should I choose something different?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I might change it to 2 TB because filming takes a lot but just in general but should i get this or get a different brand because I have heard some horror stories of it abruptly dying</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/dinoman1214\"> /u/dinoman1214 </a> <br/> <span><a href=\"https://i.redd.it/yzhtwqdkl8bg1",
        "id": 4467187,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3crk3/im_going_to_film_school_later_this_year_and_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/yzhtwqdkl8bg1.jpeg?width=640&crop=smart&auto=webp&s=088af9711331667841680efa8a368c4bbbb50cbd",
        "title": "I\u2019m going to film school later this year and I need an external SSD for large video files. Is this a good option? Or should I choose something different?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Pleasant-Lab-6775",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T02:50:56.382628+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T01:13:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hey DataHoarders,</p> <p>I\u2019m optimizing my rclone mount for encrypted OneDrive (onedrive_crypt:) on macOS. I\u2019ve fine-tuned the command for handling large data (mainly media files), and I\u2019d love your feedback on further improvements.</p> <p>Here\u2019s the command I\u2019m using:</p> <p>nohup rclone mount onedrive_crypt: ~/mount \\ --vfs-cache-mode full \\ --cache-dir &quot;$HOME/Library/Caches/rclone&quot; \\ --vfs-cache-max-size 20G \\ --vfs-cache-poll-interval 10s \\ --dir-cache-time 30m \\ --poll-interval 5m \\ --transfers 4 \\ --buffer-size 256M \\ --vfs-read-chunk-size 256M \\ --vfs-read-chunk-size-limit 1G \\ --allow-other \\ --umask 002 \\ --log-level INFO \\ --log-file &quot;$HOME/Library/Logs/rclone-mount.log&quot; \\ --use-mmap \\ --attr-timeout 10s \\ --daemon \\ --mac-mount \\ &amp;</p> <p>Key Points: Cache: 20GB for vfs-cache-max-size. Is this too high/low for a local SSD Transfers: Set to 4 parallel transfers. Is this optimal for large file management (e.g., videos)",
        "id": 4467450,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3c40d/optimizing_rclone_mount_for_encrypted_onedrive_on",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Optimizing rclone mount for Encrypted OneDrive on macOS - Feedback on Large Data Setup",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SurpriseGmg",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T00:43:21.245677+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T00:37:33+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3ba89/these_two_drives_do_not_match_despite_having/\"> <img src=\"https://preview.redd.it/j18hf2u8a8bg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8db005c7cefce1c9695330f60e62b76f11ebb82\" alt=\"These two drives do not match, despite having identical files (Windows 11). Is there anything else I can do?\" title=\"These two drives do not match, despite having identical files (Windows 11). Is there anything else I can do?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>For context, these are two backup drives that have identical data stored on each, and are both Veracrypt containers. I tend to manually copy things from the left drive here to the right one, but I&#39;ve run into an issue where despite these two drives having the exact same data on them, there&#39;s still a discrepancy here with the filesize. I&#39;ve tried resetting each recycle bin as well as FreeFileSync, but there&#39;s still a comparison i",
        "id": 4466960,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3ba89/these_two_drives_do_not_match_despite_having",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/j18hf2u8a8bg1.png?width=640&crop=smart&auto=webp&s=b8db005c7cefce1c9695330f60e62b76f11ebb82",
        "title": "These two drives do not match, despite having identical files (Windows 11). Is there anything else I can do?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Need_Not",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T00:43:21.426588+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T00:20:37+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I bought two 6TB HDDs. I am using these for archival and not everyday backups.<br/> I&#39;m going to store one offsite and swap them every few months.</p> <p>These drives will not be online all the time. If there is something important I need to read or write I&#39;ll turn it on.<br/> I am brand new to all this so what should I know?</p> <ol> <li>Which filesystem should I use?</li> <li>Should I use some software instead of copy and paste?</li> <li>How do I protect against bitrot?</li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Need_Not\"> /u/Need_Not </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3aw1o/my_drives_just_came_in_what_do_i_do_now/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3aw1o/my_drives_just_came_in_what_do_i_do_now/\">[comments]</a></span>",
        "id": 4466961,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3aw1o/my_drives_just_came_in_what_do_i_do_now",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "My drives just came in. What do I do now?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/lemongrassgrave",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-04T00:43:21.603333+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-04T00:16:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I really like a niche anime however I\u2019d like to archive or at least have a copy of all the different language dubs which are all hosted on Crunchyroll. And for reference, I have searched on Nyaa and other streaming sites and there is only JP and ENG dubs available.</p> <p>Is there any way I can somehow extract the audio/video? From what I\u2019ve seen, yt-dlp used to be able to download crunchyroll vids but it\u2019s no longer possible with DRM protection but search results were all years ago. Otherwise my other method would be figuring out on using a screen recorder to bypass to record each episode. Sorry if this seems like a silly question and thanks in advance for any input</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/lemongrassgrave\"> /u/lemongrassgrave </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q3asnh/advice_on_pulling_videosaudio_from_crunchyroll/\">[link]</a></span> &#32; <span><a ",
        "id": 4466962,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q3asnh/advice_on_pulling_videosaudio_from_crunchyroll",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Advice on pulling videos/audio from crunchyroll",
        "vote": 0
    }
]