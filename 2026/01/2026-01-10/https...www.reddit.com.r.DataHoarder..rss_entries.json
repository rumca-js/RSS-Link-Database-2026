[
    {
        "age": null,
        "album": "",
        "author": "/u/CiaIsMyWaifu",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T23:13:28.126755+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T22:06:45+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>When I search for wasted space or duplicate files using Treesize, I often think about how much more interesting it would be to be able to have a visualizer that represents everything in a 3D space as environments you could walk around in, even if it was just a city made of towers like the PS2&#39;s startup. </p> <p>I&#39;ve done some searching but haven&#39;t found anything that exists besides some 2D games that use your folder structure, which isn&#39;t what i&#39;m looking for. I want a true visualizer.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CiaIsMyWaifu\"> /u/CiaIsMyWaifu </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9h7th/a_3d_exploration_game_that_visualizes_the_content/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9h7th/a_3d_exploration_game_that_visualizes_the_content/\">[comments]</a></span>",
        "id": 4526797,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9h7th/a_3d_exploration_game_that_visualizes_the_content",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A 3D exploration game that visualizes the content of your disk",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/wyxe_905",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-11T03:13:20.795652+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T22:06:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I started with a simple joke video from youtube with yt-dlp, but it actually started before that with quod libet and my mp3s I got back from spotify, after erasing my previous mp3 library. There&#39;s history in the playlists themselves, and of course what didn&#39;t remain after the conversion to quod I downloaded as user-data from spotify to my hard drive.</p> <p>I had an idea to write a script and make it easier. yt-dlp and spotdl are, after all, console apps. After a few iterations, I had something decent for adding new music to my quod library from youtube. It was far from great though. Manual tagging, I could only do a few songs at a time, and I missed the ability on the streaming platform to just click and add. So I got to work.</p> <p>yt-dlp downloader script, spotdl tagging, since spotdl - ytm tagging is sometimes spotty, I got stricter with the input. I had the script make a backup prior to tagging. It&#39;s workable. Just today I modified i",
        "id": 4527615,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9h76k/looking_back",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Looking Back",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Emotional-Fix-5190",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T21:58:30.983968+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T21:14:27+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9fvse/made_my_own_checksum_program_with_help_from_ai/\"> <img src=\"https://b.thumbs.redditmedia.com/LwRmqQuHyeLVA7RUpa8dn_8ys-VJo7YSy_RiQQCszAA.jpg\" alt=\"Made my own checksum program with help from Ai and my basic coding knowledge.\" title=\"Made my own checksum program with help from Ai and my basic coding knowledge.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Like I said, I made this program using what I learned in school, with some help from AI. I\u2019m looking for a few beta testers. I plan to make it open source. It includes some features I was missing in other programs... for example, the checksum data has its own checksum.</p> <p>Like i said, i am only a beginner and tried my best to make the system redundant and useful. if you find any performance improvements or bugs let me know &lt;3</p> <p><a href=\"https://github.com/Feiyve97/Spigl-6\">https://github.com/Feiyve97/Spigl-6</a></p> </div><!-- SC_ON --> ",
        "id": 4526299,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9fvse/made_my_own_checksum_program_with_help_from_ai",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/LwRmqQuHyeLVA7RUpa8dn_8ys-VJo7YSy_RiQQCszAA.jpg",
        "title": "Made my own checksum program with help from Ai and my basic coding knowledge.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jcljules",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-11T03:13:19.428382+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T21:13:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hello data hoarders! I&#39;m Julian Lucas, a writer at the New Yorker, where I wrote an <a href=\"https://www.newyorker.com/news/the-lede/the-data-hoarders-resisting-trumps-purge\">article </a>about this community that some of you may remember last year. (I was very gratified by your <a href=\"https://www.reddit.com/r/DataHoarder/comments/1jb14bx/the_data_hoarders_resisting_trumps_purge_new/\">response</a>.) Now, I&#39;m working on a related piece about data loss and recovery, and thought that some of you might have interesting stories to share.</p> <p>Have you experienced drive failures that have resulted in irreparable losses? Or narrowly rescued your hoards from the clutches of oblivion, either by yourself or with the help of professionals? If so, I&#39;d love to hear about it in the comments, and will DM those whose stories I might want to quote at length. Bonus points if your story is dramatic or unusual, of course (as long as it&#39;s true) or if th",
        "id": 4527612,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9fupj/reporter_seeking_stories_of_data_loss_who_wrote",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Reporter seeking stories of data loss (who wrote that piece on r/DataHoarders)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/toptoptopper",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-11T03:13:21.277858+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T20:15:36+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve got a Synology 5-bay that\u2019s been running 6TB drives for approx 3 years with no failures. I\u2019ve now upgraded two of the bays to 24TB and everything is healthy.</p> <p>A mate has offered space in his NAS so I can Hyper Backup to him nightly. Worst case, if mine dies, I can recover directly from his machine.</p> <p>This raises a question: how important is keeping a cold spare (same model, recertified) on the shelf \u201cjust in case\u201d? I bought 5 drives originally so the plan was to keep one as a cold spare, until I realised I really need that drive to expand his NAS instead \u2014 which leaves me with no spare.</p> <p>Is keeping a matching cold spare actually worthwhile, given I have offsite nightly backups? Or is it a waste of money compared to just ordering a replacement when something dies?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/toptoptopper\"> /u/toptoptopper </a> <br/> <span><a href=\"https://www.reddit.com/r/",
        "id": 4527616,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9ecfc/nas_cold_spare_advice",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NAS Cold Spare Advice",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/askyidroppedthesoap",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T19:55:00.993937+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T19:12:50+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9cpo6/the_purchase_of_an_ssdm2_now_requires_a_mortgage/\"> <img src=\"https://b.thumbs.redditmedia.com/04Cw2SlaTHsRQzjReUFnoY8pXw49VIoqhjbjwwhJ85o.jpg\" alt=\"The purchase of an SSD/M.2 now requires a mortgage...\" title=\"The purchase of an SSD/M.2 now requires a mortgage...\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Putting the chip price increase in perspective... the WD_BLACK 850X 8TB That i bought for $800 a week ago, is now $1,980 a $1,180 increase in a week! \ud83d\ude33 shit&#39;s crazy. and just think, the forecasted 4-year scarcity is just getting started. If you guys are gamers like myself, we&#39;re about to feel the pain on GPU&#39;s as well. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/askyidroppedthesoap\"> /u/askyidroppedthesoap </a> <br/> <span><a href=\"https://www.reddit.com/gallery/1q9cpo6\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataH",
        "id": 4525805,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9cpo6/the_purchase_of_an_ssdm2_now_requires_a_mortgage",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/04Cw2SlaTHsRQzjReUFnoY8pXw49VIoqhjbjwwhJ85o.jpg",
        "title": "The purchase of an SSD/M.2 now requires a mortgage...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/cfelicio",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-11T03:13:21.491767+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T18:25:03+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9bg5e/symbion_a_p2p_cloud_backup_tool_looking_for_alpha/\"> <img src=\"https://b.thumbs.redditmedia.com/cY6LyDKNsyyhl1XOCjKQNUb0bpiKFRHpYSUQX2_ZEGs.jpg\" alt=\"Symbion - A P2P Cloud Backup Tool (looking for Alpha Testers)\" title=\"Symbion - A P2P Cloud Backup Tool (looking for Alpha Testers)\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I originally posted this on Hacker News, but didn&#39;t really gather much interest:</p> <blockquote> <p>For the last decade, the conversation around decentralized storage has been dominated by blockchain projects.</p> <p>Projects like Filecoin and Arweave have focused on solving Global Permanence by relying on Global Consensus: the entire network must validate and record the proofs of storage for every file, secured by a native token, mining rigs, and a global ledger. Highly complex, computationally expensive, and not user friendly.</p> <p>This type of architecture might serve",
        "id": 4527617,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9bg5e/symbion_a_p2p_cloud_backup_tool_looking_for_alpha",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/cY6LyDKNsyyhl1XOCjKQNUb0bpiKFRHpYSUQX2_ZEGs.jpg",
        "title": "Symbion - A P2P Cloud Backup Tool (looking for Alpha Testers)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jsrbert",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:44:54.662207+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T18:13:05+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have IDs, certificate, personal documents, office documents etc. these my own personal documents and stuff and I want to preserve them.</p> <p>Will scanning them would be right way or images will do? I have some photos too how to digitised them? Should scan or just take a photo.</p> <p>Can you folks please tell me how to go about it?</p> <p>Thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jsrbert\"> /u/jsrbert </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9b4t9/what_correct_way_to_archive_physical_documents/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9b4t9/what_correct_way_to_archive_physical_documents/\">[comments]</a></span>",
        "id": 4525474,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9b4t9/what_correct_way_to_archive_physical_documents",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What correct way to archive physical documents?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/51dux",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:44:55.526143+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T18:07:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I usually don&#39;t share Youtube videos like that but this is in my opinion one of the more interesting ones.</p> <p>Most people who do DIY servers on Youtube will either go full 3d printed plastics and/or won&#39;t provide detailed steps and documentation. Their projects also usually don&#39;t involve this big of a case and this amount of drives.</p> <p><a href=\"https://www.youtube.com/watch?v=vVI7atoAeoo\">https://www.youtube.com/watch?v=vVI7atoAeoo</a></p> <p>This guy went using metal for the case and offered a detailed plan on how to go about building itfrom scratch along with all the parts, sources and documentation.</p> <p>I am not planning to do so but I found the video interesting.</p> <p>What are your thoughts?</p> <p>I personally think he really should&#39;ve powder coated the case (as he mentions) to avoid rust but outside of that it seemed really decent.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user",
        "id": 4525476,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9aznx/he_built_a_1_petabyte_server_from_scratch",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "He Built a 1 Petabyte Server From Scratch",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dual-moon",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:44:55.023734+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T17:54:05+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9amx3/sif_a_public_domain_json_extension_for_semantic/\"> <img src=\"https://preview.redd.it/67csiumb3kcg1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9a659ea30b296197866e4a4b72dbea8780b2c7b\" alt=\"SIF: a public domain JSON extension for semantic data compression\" title=\"SIF: a public domain JSON extension for semantic data compression\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Hello fellow hoarders! We are so proud to present version 1.1 of the Semantic Interchange Format, a public domain semantic compression implementation from the Ada Research Foundation!</p> <p><a href=\"https://github.com/luna-system/ada-sif/\">https://github.com/luna-system/ada-sif/</a></p> <p>SIF is a format for semantically dense and/or semantically linked data! It achieves GREAT compression against semantically sparse data (like ENGLISH!) and was initially developed to compress system logs for devops purposes, as well as mine",
        "id": 4525475,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9amx3/sif_a_public_domain_json_extension_for_semantic",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/67csiumb3kcg1.png?width=640&crop=smart&auto=webp&s=d9a659ea30b296197866e4a4b72dbea8780b2c7b",
        "title": "SIF: a public domain JSON extension for semantic data compression",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Maineiac37",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T23:13:28.977338+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T17:34:41+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have an Orico model 9848RU3 hard drive enclosure. Originally, I placed 4 16tb HDDs in the enclosure and set up a Raid 5 using the dip switches on the enclosure. I decided to increase the total capacity of the enclosure by swapping out the 16tb HDDs for 22tb HDDs. I have completed that process by swapping out one at a time each drive. Now the Orico HW Raid Manager shows the capacity that was available with the original 16tb HDDs along with a section where it shows for each drive a certain amount of capacity that is \u201cUnreleased.\u201d It is that unreleased space which would change the capacity up to the limit of the 22tb HDDs. However, I am probably missing something obvious, but I don&#39;t see any way to have the unreleased space activated as part of the disks. Looking for help to do this. Thanks, Ken.</p> <p> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Maineiac37\"> /u/Maineiac37 </a> <br/> <span><a href=\"https",
        "id": 4526798,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9a52j/orico_getting_access_to_unreleased_space_in_raid",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Orico - Getting access to unreleased space in Raid 5 after increasing size of each disk - Model 9848RU3",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Unusual-Man5761",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T17:34:52.885804+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T16:29:29+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Advanced Forward is not working anymore. Any alternative in saving files from Telegram private channels?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Unusual-Man5761\"> /u/Unusual-Man5761 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q98ge9/aka_messenger_alternative_cuz_its_not_working/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q98ge9/aka_messenger_alternative_cuz_its_not_working/\">[comments]</a></span>",
        "id": 4525084,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q98ge9/aka_messenger_alternative_cuz_its_not_working",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AKA MESSENGER ALTERNATIVE? Cuz Its not working anymore on IOS",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/ChickenNoodleSloop",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T16:25:32.033447+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T16:04:47+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q97tns/moving_to_the_mediums_leagues_with_some_new_rust/\"> <img src=\"https://preview.redd.it/nhncmc6xojcg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=463fd48f33dac97cef018f79c70d2b6bf715e779\" alt=\"Moving to the mediums leagues with some new rust\" title=\"Moving to the mediums leagues with some new rust\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>176TB from SPD, all drives are &lt;1yo so I&#39;m interested to see their prior workload numbers over that time. I&#39;m expecting them to be very low. </p> <p>5 are going into a main Z2 pool for my extended family nextcloud and Immich, Plex, PC backups, and some archival good.</p> <p>Rest are going Z1 at my parents a few states over for offsite backup of the non-archival data.</p> <p>It&#39;s def a big upgrade over my previous 16TB pool.</p> <p>Recently side-moved from TrueNAS to Hex, mainly since I&#39;ve found it easier for my noob self to manage. Should",
        "id": 4524645,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q97tns/moving_to_the_mediums_leagues_with_some_new_rust",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/nhncmc6xojcg1.jpeg?width=640&crop=smart&auto=webp&s=463fd48f33dac97cef018f79c70d2b6bf715e779",
        "title": "Moving to the mediums leagues with some new rust",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/coffeemate1999",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T16:25:33.583110+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T15:55:21+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I am dealing with a collection of older videos where subtitles are burned into the image and the original source files are long gone.</p> <p>Manually editing each video isnt realistic at scale so i am trying to understand whether automation helps at all here. When people talk about subtitle removal how much of that is truly automatic versus partially assisted with manual review.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/coffeemate1999\"> /u/coffeemate1999 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q97ku5/what_tool_can_remove_burned_in_subtitles/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q97ku5/what_tool_can_remove_burned_in_subtitles/\">[comments]</a></span>",
        "id": 4524648,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q97ku5/what_tool_can_remove_burned_in_subtitles",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What tool can remove burned in subtitles automatically when dealing with large video archives?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Standard_Sky_4389",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T16:25:32.571810+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T15:32:46+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Just relaying this since nobody seems to have taken much notice. The classic webhost Angelfire seems to have been down for 3 days now, including their home page and all user websites. </p> <p>Hopefully a lot of it is already archived. If it does come back, just know that it may be the last chance.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Standard_Sky_4389\"> /u/Standard_Sky_4389 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9707b/angelfire_has_been_down_since_17_is_it_gone_for/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q9707b/angelfire_has_been_down_since_17_is_it_gone_for/\">[comments]</a></span>",
        "id": 4524646,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q9707b/angelfire_has_been_down_since_17_is_it_gone_for",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AngelFire has been down since 1/7, is it gone for good?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/GasNice",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T16:25:33.036878+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T15:29:33+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>So I am filmmaker, and musical artist. Im also my family archivist and same for my community. I have over 40 hard drives. </p> <p>These are older projects. My current projects needs at least 18 Tbs of storage. </p> <p>I\u2019m new to the RAID system. I have budget constraints so keep that in mind. </p> <p>What system should I get? What kind of maintenance does it need? What do you wish you knew when set up your RAID system? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/GasNice\"> /u/GasNice </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q96x8m/help_advice_on_setting_up_raid_system/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q96x8m/help_advice_on_setting_up_raid_system/\">[comments]</a></span>",
        "id": 4524647,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q96x8m/help_advice_on_setting_up_raid_system",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Help/ advice on setting up RAID system",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Mr_FuS",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T15:17:07.613025+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T14:47:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>After what it feels like an eternity finally decided to get a server MOBO to play with, ended getting a old X10DRi, I keep reading the manual and trying to make sense of things like the crazy header where the power switch and LEDs are connected for the front panel and the one million other features of the system... </p> <p>Anyway, I see that I count with multiple SATA ports, a set is label I-SATA and another group is S-SATA. I think that I understand correctly that all the drives on I-SATA can be configured as a RAID drive and the ones on S-SATA can be done equally basically resulting on two different RAID configurations.</p> <p>At this moment I&#39;m using a couple of drives that I have laying around from upgrades as my &quot;Test subjects&quot; and my main drive, didn&#39;t make any difference on what SATA group I place my main drive where UBUNTU is installed?</p> <p>And second question, did I need to go for a RAID configuration or is something that",
        "id": 4524264,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q95wfu/isata_and_ssata_on_server_mobo",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I-SATA and S-SATA on server MOBo...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/chich_bich",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T15:17:07.995936+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T14:41:18+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Choosing between two budget SATA SSD options and hoping for input from long-term users.</p> <p>The finalists are:</p> <ul> <li><strong>Seagate BarraCuda 480GB SATA SSD</strong></li> <li><strong>Verbatim Vi550 512GB SATA SSD</strong></li> </ul> <p>Online reviews show a split in reliability experiences\u2014from reports of drives failing within months to others lasting years without issue.</p> <p>For those who <strong>personally own</strong> either of these models, could you share your experience?</p> <ol> <li>Which specific model do you have?</li> <li>How long have you actively used it?</li> <li>Have you encountered any reliability problems or failures?</li> <li>Based on your experience, would you purchase it again?</li> </ol> <p>Firsthand insights on longevity are most valuable. The goal is to find a drive that reliably lasts well beyond the first year.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/chich_bich\"> /u/c",
        "id": 4524265,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q95qvk/owners_of_seagate_barracuda_or_verbatim_vi550",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Owners of Seagate BarraCuda or Verbatim Vi550 SSDs - how reliable have they been for you?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Chemical_Finding_570",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T14:08:42.800063+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T13:41:38+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Link : <a href=\"https://github.com/BMYSTERIO/IscrapeMDB\">https://github.com/BMYSTERIO/IscrapeMDB</a></p> <p>this app fetches data from IMDB (series, movie , set of movies) and extract the data so u can use it, it gets almost everything about the target -- u can even extract the data in a html local file so u can check on a IMDB series - movie if ur offline, the series option scrap the whole series and all its episodes the scraping data include Reviews , Parents Guide , cast , and more</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Chemical_Finding_570\"> /u/Chemical_Finding_570 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q94cp3/new_imdb_scraper_unlimited_data/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q94cp3/new_imdb_scraper_unlimited_data/\">[comments]</a></span>",
        "id": 4523871,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q94cp3/new_imdb_scraper_unlimited_data",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "NEW IMDB SCRAPER (UNLIMITED DATA)",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/UsedZookeepergame528",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T14:08:43.383828+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T12:38:16+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Hi everyone!</p> <p>I\u2019ve been feeling nostalgic lately and was thinking about how important Premio Lo Nuestro 2018 was for reggaeton and Latin music as a whole.</p> <p>I was wondering if anyone here knows how older Spanish-language award show broadcasts are typically preserved, or if there are communities or collectors focused on archiving Latin television history.</p> <p>I\u2019m mainly trying to understand where material like this usually ends up over time. Any guidance would be greatly appreciated. Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UsedZookeepergame528\"> /u/UsedZookeepergame528 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q931di/how_are_older_spanishlanguage_tv_broadcasts/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q931di/how_are_older_spanishlanguage_tv_broadcasts/\">[comments]</a></span>",
        "id": 4523872,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q931di/how_are_older_spanishlanguage_tv_broadcasts",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "How are older Spanish-language TV broadcasts usually preserved?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/naorunaoru",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T12:58:32.022670+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T12:10:20+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q92j3s/sticking_more_ssds_in_jonsbo_n2/\"> <img src=\"https://b.thumbs.redditmedia.com/tDbA4TuckvHwL04i2ZUlG3oLsskJdRHEk-GuYFHtkMM.jpg\" alt=\"Sticking more SSDs in Jonsbo N2\" title=\"Sticking more SSDs in Jonsbo N2\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Oh no, my expansion board won&#39;t fit! </p> <p>Anyway. Turns out there&#39;s just about enough space under the motherboard tray, however you&#39;ll need to take the case apart. </p> <p>And the drives get some airflow from the fan on the back!</p> <p>This is a motherboard from CWWK which has two SFF-8463 connectors, one for 4 SATA drives and the other is a PCIe 3.0 x4 with bifurcation, so there&#39;s just one 3.0 lane going to each SSD. I set them up as a double mirrored zfs special pool for &lt;128k files and metadata while the bulk of the stuff sits on 5x8TB raidz2 spinning rust array. </p> <p>I just think it&#39;s neat!</p> </div><!-- SC_ON --> &#32;",
        "id": 4523521,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q92j3s/sticking_more_ssds_in_jonsbo_n2",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://b.thumbs.redditmedia.com/tDbA4TuckvHwL04i2ZUlG3oLsskJdRHEk-GuYFHtkMM.jpg",
        "title": "Sticking more SSDs in Jonsbo N2",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Sluwulf",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T10:36:15.693518+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T10:28:22+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>(Preamble: its my first time learning all of this so i would appreciate if you ELI5. Ive tried to do my homework and research but im kind for looking for some guidance :( ). </p> <p>Im trying to identify the things i might want to back up, im not sure if i want to do image backups or if the program that i use for them can do them (Backrest which is a GUI for Restic) (or, if it can, if i can do it considering im not familiar with coding or command line applications). The things i identified i might care about are:</p> <p>-Personal photos, documents, videos, files (i make sure to keep these on the desktop or in C:\\Users anyways if they are not)<br/> -Game saves from Steam, Itch and small indie games, standalone games that i install, epic games (it seems these can be anywhere)<br/> -Some program configurations (seems they can also be anywhere)</p> <p>Are these usually on the program files/ windows / program data folders though? Those folders do contain a",
        "id": 4522763,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q90t9n/in_your_experience_how_often_are_these_things",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "In your experience, how often are these things outside of the C:\\Users folder in windows by default? + some other questions about image backups",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/sendcodenotnudes",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T10:36:15.950926+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T10:11:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I decided next week to consolidate my 8TB of data spread across older 1-3 TB disks. I bought 2 8TB disks and after some thinking I decided to go for a MergeFS + SnapRAID setup (8TB for the parity disk and 8+3TB for the data).</p> <p>I had a look at the parity disk: the file is 6TB.</p> <p>I am now having second thoughts about my choice of solution.</p> <p>I wanted to have a bit more than 8TB, above that I can start to clean up. I thought that I would add my old disks and maybe a new one (with time) but now I realize I have - 11 TB of data - if a disk fails, I have an interruption of service until I purhase a disk at least a big - maybe a more standard RAID would have been better, with uninterrupted activity (and some investment in disks)</p> <p>All my disks are wired directly to my server and I want it to stay that way (for several reasons, some good, some bad). My motherboard allows for 6x6 Gbps disks</p> <p>I am looking for advice. I am not in a hur",
        "id": 4522764,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q90jjv/is_snapraid_such_a_good_choice_for_me",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is SnapRAID such a good choice for me?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MrLion626",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T14:08:43.838522+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T07:26:23+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/DataHoarder/comments/1q8xupd/hello_to_all_i_have_come_into_contact_with_a/\"> <img src=\"https://preview.redd.it/88tbtndv4hcg1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7da575f9b71b495a950ecb97b4a85f3d7d558e4b\" alt=\"Hello to all! I have come into contact with a massive amount of Simpsons VHS tapes\u2014 where might I place hundreds of GBs worth of broadcasts without fears of copyright-related take-downs?\" title=\"Hello to all! I have come into contact with a massive amount of Simpsons VHS tapes\u2014 where might I place hundreds of GBs worth of broadcasts without fears of copyright-related take-downs?\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>A kind Redditor sold me thirty-four Simpsons tapes at a very reasonable price. The man recorded the series for many years, and he started out by editing the commercials away. Fortunately, though, he slowed down by Season 3, and entirely gave up doing so by Season 4. As such, most of the tap",
        "id": 4523873,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q8xupd/hello_to_all_i_have_come_into_contact_with_a",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://preview.redd.it/88tbtndv4hcg1.jpeg?width=640&crop=smart&auto=webp&s=7da575f9b71b495a950ecb97b4a85f3d7d558e4b",
        "title": "Hello to all! I have come into contact with a massive amount of Simpsons VHS tapes\u2014 where might I place hundreds of GBs worth of broadcasts without fears of copyright-related take-downs?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/SeparateFly",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T07:45:30.992031+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T06:45:34+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I have a Macbook Pro Max M4 and want an SSD I can backup things to on one partition and transfer in another. What would be the most suitable enclosure and SSD pair that is fastest for this purpose? thanks.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/SeparateFly\"> /u/SeparateFly </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q8x5oz/what_is_the_best_enclosure_and_2tb4tb_ssd_i/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q8x5oz/what_is_the_best_enclosure_and_2tb4tb_ssd_i/\">[comments]</a></span>",
        "id": 4522291,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q8x5oz/what_is_the_best_enclosure_and_2tb4tb_ssd_i",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "What is the best enclosure and 2TB/4TB SSD I should buy for a Macbook Pro M4/M5 model?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jj47336",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T06:36:22.630707+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T06:27:15+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Good day, everyone. I currently have an old Acer SFF PC setup as a media server. Here are the specs:</p> <p>Processor: i3-9100</p> <p>Motherboard: Acer proprietary (this one is very limiting as it only has 2 slots for RAM and 2 sata ports)</p> <p>RAM: 2x8gb DDR4 non-ECC</p> <p>PSU: 120w Acer proprietary (another limiting factor as there is no Sata power, my drives are being powered by a proprietary cable attached to the motherboard which is)</p> <p>HDD: Seagate Exos 8tb ZFS</p> <p>SSD: WD Green 128gb</p> <p>OS: Proxmox</p> <p>1 Ubuntu LXC with an SMB share that manages my only HDD, stores all my media</p> <p>1 Ubuntu Container with Jellyfin, Sonarr, Radarr, Prowlarr and Qbittorrent</p> <p>I have an Aerocool Strike-X One case that has 9 5.25&quot; drive bays. I plan on 3D printing drive cages that would allow me to attach 15 3.5&quot; drives on it.</p> <p>Q1: When I do buy an H310 motherboard, can I just install the i3-9100 on it, migrate my SSD and HD",
        "id": 4522054,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q8wtxi/can_i_get_advice_for_future_expansion_of_my",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Can I get advice for future expansion of my current media server?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/LongjumpingRegret326",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T03:07:46.457010+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T03:01:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Is there any alternatives ? Or do I have to download the videos the long way ? I really wish there is an alternative because if not I\u2019m going to go crazy. I\u2019m using Firefox btw. Please reply </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/LongjumpingRegret326\"> /u/LongjumpingRegret326 </a> <br/> <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q8srjq/i_have_a_samsung_tablet_bought_a_terabyte_micro/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/DataHoarder/comments/1q8srjq/i_have_a_samsung_tablet_bought_a_terabyte_micro/\">[comments]</a></span>",
        "id": 4521434,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q8srjq/i_have_a_samsung_tablet_bought_a_terabyte_micro",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "I have a Samsung tablet bought a terabyte micro SD card. Just realize video downloader helper doesn\u2019t work",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Kevalemig",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T03:07:46.590416+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T02:50:48+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I did a web search and found an old program called CDSlow.exe but it doesn&#39;t work properly, or at least I can&#39;t get it to work. Are there any other programs that can slow down an optical drive&#39;s read speed? </p> <p>The VCDs I&#39;m trying to copy is a simple drag-and-drop of the video.dat file which is an MPG file. It&#39;s CD media, not DVD or Blu Ray so the usual video programs I use lke DVD Decrypter or MakeMKV won&#39;t work with this.</p> <p>When I drag the video file over, it starts copying, but at some point the drive will speed up and the video will stall, so I have to cancel the transfer, usually having to disconnect the drive, and then Windows will have a stuck process so I can&#39;t use the optical drive again unless I force shut down Windows and start up again. It won&#39;t restart because it still thinks the optical drive is still connected and the disc is still there.</p> <p>Anyone have any tips?</p> </div><!-- SC_ON --> &#32",
        "id": 4521435,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q8sj41/windows_11_pioneer_bdr_optical_drive_connected",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Windows 11, Pioneer BDR optical drive connected via usb. Trying to rip old VCD (Video CD discs) burned 20 years ago. Any software that can slow down the read speed?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Awkward_System9248",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T03:07:46.797000+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T02:21:03+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I\u2019ve been using basic storage solutions for years (external drives + cloud), and I\u2019m finally ready to move to a proper NAS. The problem is\u2026 I\u2019m kinda stuck at the decision stage.</p> <p>My core needs are backing up photos/videos from multiple devices, organizing media better, and keeping important work files in one place. Reliability and ease of use matter a lot to me. But lately I\u2019ve been seeing more NAS brands pushing AI features, and I\u2019m not sure if that\u2019s something I should take into consideration.</p> <p>I saw Ugreen showcase their AI NAS lineup at CES. Things like smarter photo organization, face/object recognition, and local AI features sounds cool on paper. But since it\u2019s not out yet, I\u2019m wondering if waiting actually makes sense, or I\u2019d be better off buying something mature now.</p> <p>Other options I\u2019m considering:</p> <ul> <li>Synology: feels like the safe choice, solid ecosystem, but anything \u201csmart\u201d seems to require extra setup or third-p",
        "id": 4521436,
        "language": "en",
        "link": "https://www.reddit.com/r/DataHoarder/comments/1q8rvko/stuck_choosing_my_first_nas_is_ai_nas_actually",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 596,
        "source_url": "https://www.reddit.com/r/DataHoarder/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Stuck Choosing My First NAS. Is \u201cAI NAS\u201d Actually Worth Waiting For?",
        "vote": 0
    }
]