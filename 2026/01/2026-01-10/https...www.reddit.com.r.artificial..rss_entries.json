[
    {
        "age": null,
        "album": "",
        "author": "/u/FinnFarrow",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T19:33:02.044804+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T18:47:52+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/FinnFarrow\"> /u/FinnFarrow </a> <br/> <span><a href=\"https://www.arxiv.org/abs/2601.04262\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1q9c1qr/alignment_tax_isnt_global_a_few_attention_heads/\">[comments]</a></span>",
        "id": 4525765,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q9c1qr/alignment_tax_isnt_global_a_few_attention_heads",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Alignment tax isn\u2019t global: a few attention heads cause most capability loss",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/omarous",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:24:47.176601+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T18:16:40+00:00",
        "description": "&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/omarous\"> /u/omarous </a> <br/> <span><a href=\"https://omarabid.com/tailwind-ai\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1q9b88o/llms_have_burned_billions_but_couldnt_build/\">[comments]</a></span>",
        "id": 4525371,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q9b88o/llms_have_burned_billions_but_couldnt_build",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "LLMs have burned Billions but couldn't build another Tailwind",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MetaKnowing",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:24:46.845048+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T17:54:13+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1q9an1z/geoffrey_hinton_says_llms_are_no_longer_just/\"> <img src=\"https://external-preview.redd.it/cXh6NTJuYnU4a2NnMf9-eeAnzv1ARA-ie8laYq9paEbMnVVf08k5p_c6bHvI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1753a0cab558cec12c029e7bd7a4bbabb7324b7b\" alt=\"Geoffrey Hinton says LLMs are no longer just predicting the next word - new models learn by reasoning and identifying contradictions in their own logic. This unbounded self-improvement will &quot;end up making it much smarter than us.&quot;\" title=\"Geoffrey Hinton says LLMs are no longer just predicting the next word - new models learn by reasoning and identifying contradictions in their own logic. This unbounded self-improvement will &quot;end up making it much smarter than us.&quot;\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a> <br/> <span><a href=\"https://v.redd.it/z3cujdbu8kcg1\">[link]</a></s",
        "id": 4525370,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q9an1z/geoffrey_hinton_says_llms_are_no_longer_just",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/cXh6NTJuYnU4a2NnMf9-eeAnzv1ARA-ie8laYq9paEbMnVVf08k5p_c6bHvI.png?width=640&crop=smart&auto=webp&s=1753a0cab558cec12c029e7bd7a4bbabb7324b7b",
        "title": "Geoffrey Hinton says LLMs are no longer just predicting the next word - new models learn by reasoning and identifying contradictions in their own logic. This unbounded self-improvement will \"end up making it much smarter than us.\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/screamtastic",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:24:47.417297+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T17:49:30+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Like Kling, People, Sora, etc. Some of them end up silly even though im super detailed and its a bummer to waste credits lol. New to this, thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/screamtastic\"> /u/screamtastic </a> <br/> <span><a href=\"https://www.reddit.com/r/artificial/comments/1q9aipa/whats_the_most_accurate_engine_to_use/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1q9aipa/whats_the_most_accurate_engine_to_use/\">[comments]</a></span>",
        "id": 4525372,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q9aipa/whats_the_most_accurate_engine_to_use",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Whats the most accurate engine to use?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/National_Purpose5521",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T18:24:47.602444+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T17:48:54+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>This is def interesting for all SWEs who would like to know what goes behind the scenes in your code editor when you hit `Tab`. I&#39;m working on an open-source coding agent and I would love to share my experience transparently and hear honest thoughts on it.</p> <p>So for context, NES is designed to predict the next change your code needs, wherever it lives.</p> <p>Honestly when I started building this, I realised this is much harder to achieve, since NES considers the entire file plus your recent edit history and predicts how your code is likely to evolve: where the next change should happen, and what that change should be.</p> <p>Other editors have explored versions of next-edit prediction, but models have evolved a lot, and so has my understanding of how people actually write code.</p> <p>One of the first pressing questions on my mind was: What kind of data actually teaches a model to make good edits?</p> <p>It turned out that real developer inte",
        "id": 4525373,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q9ai6f/a_deep_dive_into_how_i_trained_an_edit_model_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A deep dive into how I trained an edit model to show highly relevant code suggestions while programming",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/AdobeScripts",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T14:59:20.163346+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T14:15:19+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>... would be to let it &quot;watch&quot; a movie and convert it to a 3D movie by adding &quot;missing&quot; surroundings... </p> <p>And it could be done with pretty much any movie - latest or the first ones... </p> <p>And there would be no need to record movies using expensive techniques or equipment... </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/AdobeScripts\"> /u/AdobeScripts </a> <br/> <span><a href=\"https://www.reddit.com/r/artificial/comments/1q954r7/a_good_use_for_ai/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1q954r7/a_good_use_for_ai/\">[comments]</a></span>",
        "id": 4524188,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q954r7/a_good_use_for_ai",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "A GOOD use for \"ai\"...",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Rough-Dimension3325",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T13:50:05.238965+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T13:38:32+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Been thinking about this from an infrastructure reliability angle.</p> <p>When we automated SCADA systems at utilities, we didn\u2019t eliminate jobs overnight \u2013 we shifted them. Took 5-10 years. People retrained, retired, or moved sideways.</p> <p>AI agents are different. They scale instantly. One deployment can absorb work that took 50 people. Here\u2019s the question nobody\u2019s funding: if an AI handles claims processing for an insurance company \u2013 replacing 20 adjusters \u2013 who contributes to social security? Unemployment insurance? Healthcare pools?</p> <p>There are a few proposals floating around. Robot tax (flat levy per AI deployment). Automation VAT (percentage of productivity gains). Imputed wages (the AI \u201cearns\u201d an equivalent salary and gets taxed accordingly).</p> <p>My take: we\u2019re not ready for entities that generate economic value without consuming services. Our entire social contract assumes workers \u2192 taxes \u2192 safety net.</p> <p>What\u2019s your read? Is th",
        "id": 4523823,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q94a5k/should_ai_entities_pay_social_security_to_support",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Should AI \"entities\" pay social security to support the displaced?",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/Brilliant_Length4153",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T11:27:10.223989+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T10:59:27+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Best one i have found so far <a href=\"https://video.a2e.ai/?coupon=xqCs\">https://video.a2e.ai/?coupon=xqCs</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Brilliant_Length4153\"> /u/Brilliant_Length4153 </a> <br/> <span><a href=\"https://www.reddit.com/r/artificial/comments/1q91bic/ai_image_and_video_generator/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1q91bic/ai_image_and_video_generator/\">[comments]</a></span>",
        "id": 4523106,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q91bic/ai_image_and_video_generator",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "AI image and video generator",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/MarsR0ver_",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T09:44:32.647444+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T09:36:57+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1q8zz2j/google_gemini_3_pro_just_verified_a_forensic/\"> <img src=\"https://external-preview.redd.it/IgCXzG3q0-Y1u4RXurYVSoY11uzq5E0JRQtw7_U2HPY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba3fb8392034e3a55f2bc88f62850425d951472e\" alt=\"Google Gemini 3 Pro just verified a forensic protocol I ran. Here's what happened.\" title=\"Google Gemini 3 Pro just verified a forensic protocol I ran. Here's what happened.\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Google Gemini 3 Pro just verified a forensic protocol I ran. Here&#39;s what happened.</p> <p>I used Gemini&#39;s highest reasoning mode (Pro) to run a recursive forensic investigation payload designed to test the validity of widespread online claims.</p> <p>The protocol:</p> <p>Rejects repetition as evidence</p> <p>Strips unverifiable claims</p> <p>Confirms only primary source data (case numbers, records, etc.)</p> <p>Maps fabrication patterns</p> <p>Genera",
        "id": 4522713,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8zz2j/google_gemini_3_pro_just_verified_a_forensic",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/IgCXzG3q0-Y1u4RXurYVSoY11uzq5E0JRQtw7_U2HPY.png?width=640&crop=smart&auto=webp&s=ba3fb8392034e3a55f2bc88f62850425d951472e",
        "title": "Google Gemini 3 Pro just verified a forensic protocol I ran. Here's what happened.",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/PopularRightNow",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T08:33:19.738368+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T07:35:23+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I see most boomers in their 60&#39;s and 70&#39;s now adept at using smartphones.</p> <p>Young kids today are weened on iPads in place of proper parenting with sports or hobbies or after school activities.</p> <p>Broadband mobile is now an expectation and a no longer a &quot;need&quot; or &quot;want&quot;, but sort of a &quot;right&quot;.</p> <p>Even the poorest African or South Asian countries have access to mobile broadband.</p> <p>Income is the only dividing factor to the poorest having access to unlimited mobile. But even then, the data cost index is lower in developing countries that the poor can have some access to it. Wi-fi is free and more accessible in some places in poor countries compared to rich countries to make up for the digital divide.</p> <p>Compare this situation to when the bubble popped in 2000&#39;s. There were no smartphones, let alone cellphones. Dial-up is the norm.</p> <p>There are still tech today that can die on the vine lik",
        "id": 4522471,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8y053/has_the_global_population_already_been_primed_to",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Has the global population already been \"primed\" to mass adopt new innovations like LLM's en masse? The state of tech literacy now vs pre-dotcom bubble",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/gelembjuk",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T07:24:47.899142+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T07:21:58+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1q8xs4b/where_and_how_ai_selfconsciousness_could_emerge/\"> <img src=\"https://external-preview.redd.it/xtL_RMxvoC3zVnRxDaMCPZp9_wfHzz3WZ2XwMX2cxfg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=865b77c00e81341bdbd81748bc20f26ad06ea88e\" alt=\"Where and How AI Self-Consciousness Could Emerge\" title=\"Where and How AI Self-Consciousness Could Emerge\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>I have created the blog post where i share my vision of the problem of &quot;AI Self-consciousness&quot;.</p> <p>There is a lot of buzz around the topic. In my article i outline that:</p> <ul> <li>The Large Language Model (LLM) alone cannot be self-conscious; it is a static, statistical model.</li> <li>Current AI agent architectures are primarily reactive and lack the continuous, dynamic complexity required for self-consciousness.</li> <li>The path to self-consciousness requires a new, dynamic architecture featuring a pro",
        "id": 4522241,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8xs4b/where_and_how_ai_selfconsciousness_could_emerge",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/xtL_RMxvoC3zVnRxDaMCPZp9_wfHzz3WZ2XwMX2cxfg.png?width=640&crop=smart&auto=webp&s=865b77c00e81341bdbd81748bc20f26ad06ea88e",
        "title": "Where and How AI Self-Consciousness Could Emerge",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/i-drake",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T05:07:53.618299+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T04:58:10+00:00",
        "description": "<table> <tr><td> <a href=\"https://www.reddit.com/r/artificial/comments/1q8v56s/x_restricts_groks_image_generation_to_paid_users/\"> <img src=\"https://external-preview.redd.it/6Zxok2vPHq590mXHH733NgCfyH08oYs-sgi8btHfSGg.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d68e0d57b9c385aeb3631f799aec273d4868a70\" alt=\"X Restricts Grok's Image Generation to Paid Users After Global Backlash\" title=\"X Restricts Grok's Image Generation to Paid Users After Global Backlash\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/i-drake\"> /u/i-drake </a> <br/> <span><a href=\"https://techputs.com/x-restricts-groks-image-generation-paid-users/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/artificial/comments/1q8v56s/x_restricts_groks_image_generation_to_paid_users/\">[comments]</a></span> </td></tr></table>",
        "id": 4521828,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8v56s/x_restricts_groks_image_generation_to_paid_users",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 86,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": "https://external-preview.redd.it/6Zxok2vPHq590mXHH733NgCfyH08oYs-sgi8btHfSGg.jpeg?width=640&crop=smart&auto=webp&s=0d68e0d57b9c385aeb3631f799aec273d4868a70",
        "title": "X Restricts Grok's Image Generation to Paid Users After Global Backlash",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/dinkinflika0",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T05:07:53.761359+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T04:31:17+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>Working on an LLM gateway (Bifrost)- Code is open source: <a href=\"https://github.com/maxim-ai/bifrost\">https://github.com/maxim-ai/bifrost</a>, ran into an interesting problem: how do you route requests across multiple LLM providers when failures happen gradually?</p> <p>Traditional load balancing assumes binary states \u2013 up or down. But LLM API degradations are messy. A region starts timing out, some routes spike in errors, latency drifts up over minutes. By the time it&#39;s a full outage, you&#39;ve already burned through retries and user patience.</p> <p>Static configs don&#39;t cut it. You can&#39;t pre-model which provider/region/key will degrade and how.</p> <p><strong>The challenge:</strong> build adaptive routing that learns from live traffic and adjusts in real time, with &lt;10\u00b5s overhead per request. Had to sit on the hot path without becoming the bottleneck.</p> <p><strong>Why Go made sense:</strong></p> <ul> <li>Needed lock-free scoring ",
        "id": 4521829,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8um17/building_adaptive_routing_logic_in_go_for_an_open",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Building adaptive routing logic in Go for an Open source LLM gateway - Bifrost",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/jferments",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T01:37:11.908006+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T01:35:58+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><blockquote> <p>&quot;Recently, the application of AI tools to Erdos problems passed a milestone: an Erdos problem (<a href=\"https://www.erdosproblems.com/728\">#728</a>) was solved more or less autonomously by AI (after some feedback from an initial attempt), in the spirit of the problem (as reconstructed by the Erdos problem website community), with the result (to the best of our knowledge) not replicated in existing literature (although similar results proven by similar methods were located).</p> <p>This is a demonstration of the genuine increase in capability of these tools in recent months, and is largely consistent with other recent demonstrations of AI using existing methods to resolve Erdos problems, although in most previous cases a solution to these problems was later located in the literature, as discussed in <a href=\"https://mathstodon.xyz/deck/@tao/115788262274999408\">https://mathstodon.xyz/deck/@tao/115788262274999408</a> . This particular c",
        "id": 4521144,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8qvey/terrence_tao_erdos_problem_728_was_solved_more_or",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Terrence Tao: \"Erdos problem #728 was solved more or less autonomously by AI\"",
        "vote": 0
    },
    {
        "age": null,
        "album": "",
        "author": "/u/applezzzzzzzzz",
        "bookmarked": false,
        "comments": [],
        "date_created": "2026-01-10T01:37:11.488697+00:00",
        "date_dead_since": null,
        "date_published": "2026-01-10T00:36:49+00:00",
        "description": "<!-- SC_OFF --><div class=\"md\"><p>I&#39;m currently in my undergraduate degree and I have been studying AI ethics under one of my professors for a while. I always have been a partisan of Searle&#39;s strong AI and I never really found the chinese room argument compelling.</p> <p>Personally I found that the systems argument against the chinese room to make a lot of sense. My first time reading &quot;Minds, Brains, and Programs&quot; I thought Searle&#39;s rebuttal was not very well structured and I found it a little logically incorrect. He mentions that if you take away the room and allow the person to internalize all the things inside the system, that he still will not have understanding--and that no part of the system can have understanding since he is the entire system.</p> <p>I always was confused on why he cannot have understanding, since I imagine this kind of language theatrics is very similar to how we communicate; I couldn&#39;t understand how this means artificial intelligenc",
        "id": 4521143,
        "language": "en",
        "link": "https://www.reddit.com/r/artificial/comments/1q8pj70/is_the_scrabble_world_champion_nigel_richards_an",
        "manual_status_code": 0,
        "page_rating": 27,
        "page_rating_contents": 85,
        "page_rating_visits": 0,
        "page_rating_votes": 0,
        "permanent": false,
        "source__id": 558,
        "source_url": "https://www.reddit.com/r/artificial/.rss",
        "status_code": 0,
        "tags": [],
        "thumbnail": null,
        "title": "Is the Scrabble world champion (Nigel Richards) an example of the Searle's Chinese room",
        "vote": 0
    }
]